#!/usr/bin/env python3
"""
é›†æˆæµ‹è¯• processed_ids åŠŸèƒ½
æµ‹è¯•ä¸æ–°é—»å¤„ç†ç®¡é“çš„é›†æˆ
"""

import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.adapters.sqlite.store import get_store
from src.app.business.extraction import get_unprocessed_news_files, process_news_pipeline
from src.infra.paths import tools


def create_test_news_file():
    """åˆ›å»ºæµ‹è¯•æ–°é—»æ–‡ä»¶"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    raw_dir = tools.RAW_NEWS_TMP_DIR
    raw_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯•æ–°é—»æ•°æ®
    test_news = [
        {
            "id": "test001",
            "source": "test_source",
            "title": "Test News 1",
            "content": "This is test news content 1",
            "timestamp": "2025-12-19T10:00:00Z"
        },
        {
            "id": "test002",
            "source": "test_source",
            "title": "Test News 2",
            "content": "This is test news content 2",
            "timestamp": "2025-12-19T11:00:00Z"
        }
    ]
    
    # å†™å…¥æµ‹è¯•æ–‡ä»¶
    test_file = raw_dir / "test_news.jsonl"
    with open(test_file, "w", encoding="utf-8") as f:
        for news in test_news:
            f.write(json.dumps(news, ensure_ascii=False) + "\n")
    
    print(f"åˆ›å»ºæµ‹è¯•æ–°é—»æ–‡ä»¶: {test_file}")
    return test_file


def test_get_unprocessed_news_files():
    """æµ‹è¯•è·å–æœªå¤„ç†æ–°é—»æ–‡ä»¶åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯• get_unprocessed_news_files åŠŸèƒ½")
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_file = create_test_news_file()
    
    # è·å–æœªå¤„ç†çš„æ–°é—»æ–‡ä»¶
    unprocessed_files = get_unprocessed_news_files()
    
    print(f"æ‰¾åˆ° {len(unprocessed_files)} ä¸ªæœªå¤„ç†çš„æ–°é—»æ–‡ä»¶")
    for file in unprocessed_files:
        print(f"  - {file}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    test_file.unlink(missing_ok=True)
    
    return len(unprocessed_files) > 0


def test_processed_ids_integration():
    """æµ‹è¯• processed_ids é›†æˆåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯• processed_ids é›†æˆåŠŸèƒ½")
    
    # è·å– store å®ä¾‹
    store = get_store()
    
    # è·å–åˆå§‹å·²å¤„ç†IDæ•°é‡
    initial_ids = store.get_processed_ids()
    print(f"åˆå§‹å·²å¤„ç†IDæ•°é‡: {len(initial_ids)}")
    
    # æ·»åŠ ä¸€äº›æµ‹è¯•ID
    test_ids = [
        ("test_source:test001", "test_source", "test001"),
        ("test_source:test002", "test_source", "test002"),
    ]
    
    print(f"æ·»åŠ  {len(test_ids)} ä¸ªæµ‹è¯•ID...")
    count = store.add_processed_ids(test_ids)
    print(f"æˆåŠŸæ·»åŠ  {count} ä¸ªæ–°ID")
    
    # å†æ¬¡è·å–å·²å¤„ç†ID
    updated_ids = store.get_processed_ids()
    print(f"æ›´æ–°åå·²å¤„ç†IDæ•°é‡: {len(updated_ids)}")
    
    # éªŒè¯IDæ˜¯å¦å­˜åœ¨
    for global_id, _, _ in test_ids:
        if global_id in updated_ids:
            print(f"âœ… ID {global_id} å­˜åœ¨äºæ•°æ®åº“ä¸­")
        else:
            print(f"âŒ ID {global_id} ä¸å­˜åœ¨äºæ•°æ®åº“ä¸­")
    
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹é›†æˆæµ‹è¯•")
    
    # æµ‹è¯• processed_ids é›†æˆåŠŸèƒ½
    success1 = test_processed_ids_integration()
    
    # æµ‹è¯•è·å–æœªå¤„ç†æ–°é—»æ–‡ä»¶åŠŸèƒ½
    success2 = test_get_unprocessed_news_files()
    
    if success1 and success2:
        print("âœ… æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        
    return success1 and success2


if __name__ == "__main__":
    main()