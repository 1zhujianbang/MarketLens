# src/agents/agent1.py
"""
æ™ºèƒ½ä½“1ï¼šæµå¼æ–°é—»å»é‡ + LLMé©±åŠ¨çš„çœŸå®ä¸–ç•Œå®ä½“ä¸äº‹ä»¶æå–å™¨

æ ¸å¿ƒåŸåˆ™ï¼š
- å®ä½“ = èƒ½ç­¾ç½²åˆåŒã€è¢«èµ·è¯‰ã€å‘å¸ƒå…¬å‘Šã€æ‹¥æœ‰é“¶è¡Œè´¦æˆ·çš„ä¸»ä½“
  ï¼ˆè‡ªç„¶äººã€å…¬å¸ã€æ”¿åºœæœºæ„ã€å›½å®¶ã€åœ°åŒºã€å›½é™…ç»„ç»‡ï¼‰
- æ’é™¤ï¼šä»£å¸åç§°ã€æŠ€æœ¯æœ¯è¯­ã€æŠ½è±¡æ¦‚å¿µã€æƒ…ç»ªè¯ã€æ³›ç§°
- æå–å³è‡ªåŠ¨å†™å…¥ entities.jsonï¼Œæ— éœ€äººå·¥å®¡æ ¸
- æ¯ä¸ªäº‹ä»¶ç”Ÿæˆå”¯ä¸€æ‘˜è¦ï¼Œå¹¶å…³è”å®ä½“ä¸äº‹ä»¶æè¿°
"""

import os
import sys
import json
import re
import time
import hashlib
from pathlib import Path
from typing import List, Dict, Set
from datetime import datetime, timezone
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from dotenv import load_dotenv
from ..utils.tool_function import tools
tools = tools()
from .api_client import LLMAPIPool
API_POOL = None

def init_api_pool():
    global API_POOL
    if API_POOL is None:
        API_POOL = LLMAPIPool()


# ======================
# æ–°é—»å»é‡å™¨
# ======================

class NewsDeduplicator:
    def __init__(self, threshold: int = 3):
        self.threshold = threshold
        self.seen_hashes: Set[int] = set()

    def is_duplicate(self, text: str) -> bool:
        h = tools.simhash(text)
        for seen_h in self.seen_hashes:
            if tools.hamming_distance(h, seen_h) <= self.threshold:
                return True
        self.seen_hashes.add(h)
        return False

    def dedupe_file(self, input_path: Path, output_path: Path):
        tools.log(f"ğŸ” å»é‡ä¸­: {input_path.name}")
        seen_ids = set()
        if output_path.exists():
            with open(output_path, "r", encoding="utf-8") as f:
                for line in f:
                    item = json.loads(line)
                    seen_ids.add(item["id"])
        
        with open(input_path, "r", encoding="utf-8") as fin, \
             open(output_path, "a", encoding="utf-8") as fout:
            for line in fin:
                try:
                    news = json.loads(line)
                    if news["id"] in seen_ids:
                        continue
                    raw_text = (news.get("title", "") + " " + news.get("content", "")).strip()
                    if not raw_text:
                        continue
                    if self.is_duplicate(raw_text):
                        continue
                    fout.write(line)
                    seen_ids.add(news["id"])
                except Exception as e:
                    tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")

# ======================
# LLM ç»“æ„åŒ–æå–å™¨ï¼ˆå«ç²¾å‡†æç¤ºè¯ï¼‰
# ======================

def llm_extract_events(title: str, content: str, max_retries=2) -> List[Dict]:
    # åˆå§‹åŒ– API æ± ï¼ˆå•ä¾‹ï¼‰
    init_api_pool()
    if API_POOL is None:
        tools.log("[LLMè¯·æ±‚] âŒ API æ± æœªåˆå§‹åŒ–")
        return []

    prompt = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡‘èä¸æ³•å¾‹ä¿¡æ¯ç»“æ„åŒ–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–°é—»ä¸­æå–æ‰€æœ‰**çœŸå®å­˜åœ¨çš„ã€å…·æœ‰æ³•å¾‹äººæ ¼æˆ–è¡Œæ”¿èŒèƒ½çš„å®ä½“**ã€‚

ã€å®ä½“å®šä¹‰ã€‘
âœ… å¿…é¡»æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
- æ˜¯è‡ªç„¶äººï¼ˆå¦‚ Elon Muskã€Cathie Woodï¼‰
- æ˜¯æ³¨å†Œå…¬å¸ï¼ˆå¦‚ Binanceã€Coinbaseã€Teslaï¼‰
- æ˜¯æ”¿åºœæœºæ„æˆ–éƒ¨é—¨ï¼ˆå¦‚ ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šã€ä¸­å›½äººæ°‘é“¶è¡Œã€æ¬§ç›Ÿå§”å‘˜ä¼šï¼‰
- æ˜¯ä¸»æƒå›½å®¶æˆ–æ˜ç¡®è¡Œæ”¿åŒºï¼ˆå¦‚ ç¾å›½ã€æ–°åŠ å¡ã€åŠ åˆ©ç¦å°¼äºšå·ã€é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºï¼‰
- æ˜¯å›½é™…ç»„ç»‡ï¼ˆå¦‚ å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡ã€è”åˆå›½ï¼‰

âŒ ä»¥ä¸‹å†…å®¹**ä¸å¾—**è§†ä¸ºå®ä½“ï¼š
- æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚ â€œå»ä¸­å¿ƒåŒ–â€ã€â€œæµåŠ¨æ€§â€ã€â€œå¸‚åœºæƒ…ç»ªâ€ï¼‰
- æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚ â€œæ™ºèƒ½åˆçº¦â€ã€â€œé›¶çŸ¥è¯†è¯æ˜â€ã€â€œPoSâ€ï¼‰
- ä»£å¸/èµ„äº§åç§°ï¼ˆå¦‚ â€œBTCâ€ã€â€œä»¥å¤ªåŠâ€ã€â€œSolanaâ€ï¼‰â€”â€”é™¤éæŒ‡ä»£å…¶åŸºé‡‘ä¼šæˆ–å¼€å‘å…¬å¸ï¼ˆå¦‚ â€œä»¥å¤ªåŠåŸºé‡‘ä¼šâ€ï¼‰
- æ³›ç§°ï¼ˆå¦‚ â€œæŠ•èµ„è€…â€ã€â€œç›‘ç®¡æœºæ„â€ã€â€œæŸäº¤æ˜“æ‰€â€ï¼‰
- æƒ…ç»ª/è¡Œæƒ…æè¿°ï¼ˆå¦‚ â€œç‰›å¸‚â€ã€â€œæš´è·Œâ€ã€â€œåˆ©å¥½â€ï¼‰

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. åˆ¤æ–­æ–°é—»æ˜¯å¦åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªç‹¬ç«‹äº‹ä»¶ã€‚
2. å¯¹æ¯ä¸ªäº‹ä»¶ï¼Œè¾“å‡ºï¼š
   - ä¸€ä¸ªç®€æ´ã€å®¢è§‚ã€æ— æƒ…ç»ªçš„ä¸­æ–‡æ‘˜è¦ï¼ˆä½œä¸ºäº‹ä»¶å”¯ä¸€æ ‡è¯†ï¼‰
   - æ‰€æœ‰ç¬¦åˆä¸Šè¿°å®šä¹‰çš„å®ä½“ï¼ˆå…¨ç§°ä¼˜å…ˆï¼Œé¿å…ç¼©å†™ï¼‰
   - è¯¥äº‹ä»¶çš„æœ¬è´¨æè¿°ï¼ˆä¸€å¥è¯è¯´æ˜â€œè°å¯¹è°åšäº†ä»€ä¹ˆâ€ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡æœ¬ï¼š
{{
  "events": [
    {{
      "abstract": "ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šæ¨è¿Ÿå¯¹æ¯”ç‰¹å¸ETFçš„æœ€ç»ˆå†³å®š",
      "entities": ["ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼š", "VanEck"],
      "event_summary": "ç›‘ç®¡æœºæ„å»¶é•¿äº†å¯¹æŸèµ„äº§ç®¡ç†å…¬å¸æ¯”ç‰¹å¸ETFç”³è¯·çš„å®¡æŸ¥æœŸ"
    }}
  ]
}}

ã€æ–°é—»ã€‘
æ ‡é¢˜ï¼š{title}
æ­£æ–‡ï¼š{content}"""

    # è°ƒç”¨ API æ± 
    raw_content = API_POOL.call(
        prompt=prompt,
        max_tokens=1500,
        timeout=55,      # é¿å¼€ 60s ä»£ç†è¶…æ—¶
        retries=max_retries
    )

    if not raw_content:
        return []

    # æ¸…ç† Markdown åŒ…è£¹
    try:
        if raw_content.startswith("```json"):
            raw_content = raw_content.split("```json", 1)[1].split("```")[0]
        elif raw_content.startswith("```"):
            raw_content = raw_content.split("```", 1)[1].split("```")[0]

        data = json.loads(raw_content)
        events = data.get("events", [])
        result = []
        for item in events:
            abstract = item.get("abstract", "").strip()
            entities = [e for e in item.get("entities", []) if tools.is_valid_entity(e)]
            summary = item.get("event_summary", "").strip()
            if abstract and entities and summary:
                result.append({
                    "abstract": abstract,
                    "entities": entities,
                    "event_summary": summary
                })
        return result
    except Exception as e:
        tools.log(f"[LLMè·å–] âŒ LLM è¿”å›å†…å®¹è§£æå¤±è´¥: {e}")
        return []
    
# ======================
# è‡ªåŠ¨æ›´æ–°çŸ¥è¯†åº“
# ======================

def update_entities(entities: List[str], source: str):
    """è‡ªåŠ¨å†™å…¥ä¸»å®ä½“åº“"""
    now = datetime.now(timezone.utc).isoformat()
    existing = {}
    if tools.ENTITIES_FILE.exists():
        with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
            existing = json.load(f)
    
    for ent in entities:
        if ent not in existing:
            existing[ent] = {
                "first_seen": now,
                "sources": [source]
            }
        else:
            if source not in existing[ent]["sources"]:
                existing[ent]["sources"].append(source)
    
    with open(tools.ENTITIES_FILE, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)

def update_abstract_map(extracted_list: List[Dict], source: str):
    abstract_map = {}
    if tools.ABSTRACT_MAP_FILE.exists():
        with open(tools.ABSTRACT_MAP_FILE, "r", encoding="utf-8") as f:
            abstract_map = json.load(f)
    
    now = datetime.now(timezone.utc).isoformat()
    for item in extracted_list:
        key = item["abstract"]
        if key not in abstract_map:
            abstract_map[key] = {
                "entities": item["entities"],
                "event_summary": item["event_summary"],
                "sources": [source],
                "first_seen": now
            }
        else:
            s_set = set(abstract_map[key]["sources"])
            s_set.add(source)
            abstract_map[key]["sources"] = sorted(s_set)
    
    with open(tools.ABSTRACT_MAP_FILE, "w", encoding="utf-8") as f:
        json.dump(abstract_map, f, ensure_ascii=False, indent=2)

# ======================
# ä¸»å¤„ç†æµç¨‹
# ======================

def get_unprocessed_news_files() -> List[Path]:
    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r") as f:
            processed_ids = set(line.strip() for line in f if line.strip())
    
    unprocessed = []
    for raw_file in sorted(tools.RAW_NEWS_DIR.glob("*.jsonl")):
        deduped_file = tools.DEDUPED_NEWS_DIR / f"{raw_file.stem}_deduped.jsonl"
        if not deduped_file.exists():
            deduper = NewsDeduplicator(threshold=tools.DEDUPE_THRESHOLD)
            deduper.dedupe_file(raw_file, deduped_file)
        unprocessed.append(deduped_file)
    return unprocessed

def process_news_stream():
    tools.log("ğŸš€ å¯åŠ¨ Agent1ï¼šæµå¼äº‹ä»¶ä¸çœŸå®å®ä½“æå–...")
    files = get_unprocessed_news_files()
    if not files:
        tools.log("ğŸ“­ æ— å¯å¤„ç†æ–°é—»æ–‡ä»¶")
        return

    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r") as f:
            processed_ids = set(line.strip() for line in f if line.strip())

    total_processed = 0
    with open(tools.PROCESSED_IDS_FILE, "a") as id_log:
        for file_path in files:
            tools.log(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news = json.loads(line)
                        news_id = news["id"]
                        if news_id in processed_ids:
                            continue

                        title = news.get("title", "")
                        content = news.get("content", "")
                        source = news.get("source", "unknown")

                        extracted = llm_extract_events(title, content)

                        # åªæœ‰æˆåŠŸæå–åˆ°æœ‰æ•ˆäº‹ä»¶ï¼Œæ‰è§†ä¸ºâ€œå·²å¤„ç†â€
                        if extracted:
                            all_entities = []
                            for ev in extracted:
                                all_entities.extend(ev["entities"])
                            if all_entities:
                                update_entities(all_entities, source)
                                update_abstract_map(extracted, source)
                                total_processed += 1

                                # âœ… ä»…åœ¨æ­¤å¤„è®°å½•ä¸ºå·²å¤„ç†ï¼
                                id_log.write(news_id + "\n")
                                processed_ids.add(news_id)
                            else:
                                tools.log(f"ğŸ” æ–°é—» {news_id}ï¼šLLM è¿”å›äº‹ä»¶ä½†æ— æœ‰æ•ˆå®ä½“ï¼Œæš‚ä¸æ ‡è®°")
                        else:
                            tools.log(f"â³ æ–°é—» {news_id}ï¼šLLM æœªè¿”å›æœ‰æ•ˆäº‹ä»¶ï¼Œä¿ç•™é‡è¯•æœºä¼š")

                    except Exception as e:
                        tools.log(f"âš ï¸ å¤„ç†å•æ¡æ–°é—»å¤±è´¥: {e}")

             
    tools.log(f"âœ… å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡å«æœ‰æ•ˆå®ä½“çš„æ–°é—»")
    

# ======================
# å…¥å£
# ======================

if __name__ == "__main__":
    process_news_stream()