# src/agents/agent1.py
"""
æ™ºèƒ½ä½“1ï¼šæµå¼æ–°é—»å»é‡ + LLMé©±åŠ¨çš„çœŸå®ä¸–ç•Œå®ä½“ä¸äº‹ä»¶æå–å™¨

æ ¸å¿ƒåŸåˆ™ï¼š
- å®ä½“ = èƒ½ç­¾ç½²åˆåŒã€è¢«èµ·è¯‰ã€å‘å¸ƒå…¬å‘Šã€æ‹¥æœ‰é“¶è¡Œè´¦æˆ·çš„ä¸»ä½“
  ï¼ˆè‡ªç„¶äººã€å…¬å¸ã€æ”¿åºœæœºæ„ã€å›½å®¶ã€åœ°åŒºã€å›½é™…ç»„ç»‡ï¼‰
- æ’é™¤ï¼šä»£å¸åç§°ã€æŠ€æœ¯æœ¯è¯­ã€æŠ½è±¡æ¦‚å¿µã€æƒ…ç»ªè¯ã€æ³›ç§°
- æå–å³è‡ªåŠ¨å†™å…¥ entities.jsonï¼Œæ— éœ€äººå·¥å®¡æ ¸
- æ¯ä¸ªäº‹ä»¶ç”Ÿæˆå”¯ä¸€æ‘˜è¦ï¼Œå¹¶å…³è”å®ä½“ä¸äº‹ä»¶æè¿°
- è‡ªåŠ¨æ›´æ–°çŸ¥è¯†å›¾è°±ï¼Œç»´æŠ¤å®ä½“-äº‹ä»¶å…³ç³»ç½‘ç»œ
"""

import os
import sys
import json
import re
import time
import hashlib
from pathlib import Path
from typing import List, Dict, Set, Optional
from datetime import datetime, timezone
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from dotenv import load_dotenv
from ..utils.tool_function import tools
tools = tools()
from .api_client import LLMAPIPool
from ..utils.entity_updater import update_entities, update_abstract_map
from .agent3 import refresh_graph
API_POOL = None

def init_api_pool():
    global API_POOL
    if API_POOL is None:
        API_POOL = LLMAPIPool()


# ======================
# æ–°é—»å»é‡å™¨
# ======================

class NewsDeduplicator:
    def __init__(self, threshold: int = 3):
        self.threshold = threshold
        self.seen_hashes: Set[int] = set()

    @staticmethod
    def _news_key(news: Dict) -> str:
        """æ„é€ ç”¨äºå»é‡çš„å”¯ä¸€é”®ï¼ŒåŒ…å« source å‰ç¼€ï¼Œå…¼å®¹å¤šæ•°æ®æºã€‚"""
        return f"{news.get('source', 'unknown')}:{news.get('id')}"

    def is_duplicate(self, text: str) -> bool:
        h = tools.simhash(text)
        for seen_h in self.seen_hashes:
            if tools.hamming_distance(h, seen_h) <= self.threshold:
                return True
        self.seen_hashes.add(h)
        return False

    def dedupe_file(self, input_path: Path, output_path: Path, processed_ids: Optional[Set[str]] = None):
        """
        å¯¹å•ä¸ªåŸå§‹æ–‡ä»¶åšå»é‡ï¼š
        - å…ˆç”¨ processed_idsï¼ˆå…¨å±€å·²å¤„ç† IDï¼Œå¦‚ blockbeats:323066ï¼‰è¿‡æ»¤å†å²å·²å¤„ç†æ–°é—»
        - å†ç»“åˆå·²æœ‰å»é‡æ–‡ä»¶ & simhash å»æ‰æœ¬æ‰¹å†…/è·¨æ‰¹çš„é‡å¤å†…å®¹
        """
        tools.log(f"ğŸ” å»é‡ä¸­: {input_path.name}")

        # å…ˆåŠ è½½â€œå…¨å±€å·²å¤„ç† IDâ€ï¼Œé¿å…è€æ–°é—»å†æ¬¡è¿›å…¥å»é‡ç»“æœ
        seen_ids: Set[str] = set(processed_ids or set())
        if processed_ids:
            tools.log(f"ğŸ” å·²æœ‰å†å² processed_ids æ•°é‡: {len(processed_ids)}")

        # å†åŠ è½½å·²æœ‰å»é‡ç»“æœæ–‡ä»¶ä¸­çš„ IDï¼Œå®ç°è·¨æ‰¹æ¬¡çš„æœ¬åœ°å»é‡
        if output_path.exists():
            with open(output_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        item = json.loads(line)
                        seen_ids.add(self._news_key(item))
                    except Exception as e:
                        tools.log(f"âš ï¸ è¯»å–å†å²å»é‡æ–‡ä»¶æ—¶è·³è¿‡æ— æ•ˆè¡Œ: {e}")
        
        kept, skipped_id, skipped_sim = 0, 0, 0
        with open(input_path, "r", encoding="utf-8") as fin, \
             open(output_path, "a", encoding="utf-8") as fout:
            for line in fin:
                try:
                    news = json.loads(line)
                    key = self._news_key(news)

                    # 1) æŒ‰å…¨å±€ ID å»é‡ï¼ˆåŒ…æ‹¬ processed_ids å’Œå·²æœ‰å»é‡æ–‡ä»¶ä¸­çš„ IDï¼‰
                    if key in seen_ids:
                        skipped_id += 1
                        continue

                    # 2) æ„é€ æ–‡æœ¬ï¼ŒæŒ‰å†…å®¹ç›¸ä¼¼åº¦å»é‡
                    raw_text = (news.get("title", "") + " " + news.get("content", "")).strip()
                    if not raw_text:
                        continue
                    if self.is_duplicate(raw_text):
                        skipped_sim += 1
                        continue
                    fout.write(line)
                    seen_ids.add(key)
                    kept += 1
                except Exception as e:
                    tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
        tools.log(f"âœ… å»é‡å®Œæˆ: ä¿ç•™ {kept} æ¡, æŒ‰ ID è·³è¿‡ {skipped_id} æ¡, æŒ‰ç›¸ä¼¼åº¦è·³è¿‡ {skipped_sim} æ¡")

# ======================
# LLM ç»“æ„åŒ–æå–å™¨ï¼ˆå«ç²¾å‡†æç¤ºè¯ï¼‰
# ======================

def llm_extract_events(title: str, content: str, max_retries=2) -> List[Dict]:
    # åˆå§‹åŒ– API æ± ï¼ˆå•ä¾‹ï¼‰
    init_api_pool()
    if API_POOL is None:
        tools.log("[LLMè¯·æ±‚] âŒ API æ± æœªåˆå§‹åŒ–")
        return []

    prompt = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡‘èä¸æ³•å¾‹ä¿¡æ¯ç»“æ„åŒ–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–°é—»ä¸­æå–æ‰€æœ‰**çœŸå®å­˜åœ¨çš„ã€å…·æœ‰æ³•å¾‹äººæ ¼æˆ–è¡Œæ”¿èŒèƒ½çš„å®ä½“**ã€‚

ã€å®ä½“å®šä¹‰ã€‘
âœ… å¿…é¡»æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
- æ˜¯è‡ªç„¶äººï¼ˆå¦‚ Elon Muskã€Cathie Woodã€Warren Buffettï¼‰
- æ˜¯æ³¨å†Œå…¬å¸ï¼ˆå¦‚ Apple Inc.ã€Goldman Sachsã€ä¸­å›½å·¥å•†é“¶è¡Œã€Volkswagen AGï¼‰
- æ˜¯æ”¿åºœæœºæ„æˆ–éƒ¨é—¨ï¼ˆå¦‚ ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šã€ä¸­å›½äººæ°‘é“¶è¡Œã€æ¬§ç›Ÿå§”å‘˜ä¼šã€æ—¥æœ¬é‡‘èå…ï¼‰
- æ˜¯ä¸»æƒå›½å®¶æˆ–æ˜ç¡®è¡Œæ”¿åŒºï¼ˆå¦‚ ç¾å›½ã€æ–°åŠ å¡ã€åŠ åˆ©ç¦å°¼äºšå·ã€é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºã€å¾·æ„å¿—è”é‚¦å…±å’Œå›½ï¼‰
- æ˜¯å›½é™…ç»„ç»‡ï¼ˆå¦‚ å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡ã€ä¸–ç•Œé“¶è¡Œã€è”åˆå›½ã€é‡‘èç¨³å®šç†äº‹ä¼šï¼‰

âŒ ä»¥ä¸‹å†…å®¹**ä¸å¾—**è§†ä¸ºå®ä½“ï¼š
- æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚ â€œå¸‚åœºæ³¢åŠ¨â€ã€â€œç³»ç»Ÿæ€§é£é™©â€ã€â€œèµ„æœ¬æµåŠ¨â€ï¼‰
- æŠ€æœ¯æˆ–é‡‘èæœ¯è¯­ï¼ˆå¦‚ â€œæœŸæƒå®šä»·â€ã€â€œèµ„äº§è´Ÿå€ºè¡¨â€ã€â€œé‡åŒ–å®½æ¾â€ï¼‰
- é‡‘èå·¥å…·æˆ–èµ„äº§åç§°ï¼ˆå¦‚ â€œæ ‡æ™®500æŒ‡æ•°â€ã€â€œ10å¹´æœŸç¾å€ºâ€ã€â€œé»„é‡‘æœŸè´§â€ã€â€œBTCâ€ï¼‰â€”â€”é™¤éæŒ‡ä»£å…¶å‘è¡Œæ–¹ã€ç®¡ç†æ–¹æˆ–å…³è”æ³•äººï¼ˆå¦‚ â€œæ ‡æ™®é“ç¼æ–¯æŒ‡æ•°å…¬å¸â€ï¼‰
- æ³›ç§°ï¼ˆå¦‚ â€œæŠ•èµ„è€…â€ã€â€œç›‘ç®¡æœºæ„â€ã€â€œæŸé“¶è¡Œâ€ã€â€œå¤§å‹ç§‘æŠ€å…¬å¸â€ï¼‰
- æƒ…ç»ª/è¡Œæƒ…æè¿°ï¼ˆå¦‚ â€œæš´æ¶¨â€ã€â€œæŠ›å”®æ½®â€ã€â€œç»æµè¡°é€€æ‹…å¿§â€ï¼‰

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. åˆ¤æ–­æ–°é—»æ˜¯å¦åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªç‹¬ç«‹äº‹ä»¶ã€‚
2. å¯¹æ¯ä¸ªäº‹ä»¶ï¼Œè¾“å‡ºï¼š
   - ä¸€ä¸ªç®€æ´ã€å®¢è§‚ã€æ— æƒ…ç»ªçš„ä¸­æ–‡æ‘˜è¦ï¼ˆä½œä¸ºäº‹ä»¶å”¯ä¸€æ ‡è¯†ï¼‰
   - æ‰€æœ‰ç¬¦åˆä¸Šè¿°å®šä¹‰çš„å®ä½“ï¼ˆå…¨ç§°ä¼˜å…ˆï¼Œé¿å…ç¼©å†™ï¼›è‹¥åŸæ–‡ä½¿ç”¨è‹±æ–‡åä¸”æ— é€šç”¨ä¸­æ–‡è¯‘åï¼Œåˆ™ä¿ç•™è‹±æ–‡ï¼‰
   - æ‰€æœ‰ç¬¦åˆä¸Šè¿°å®šä¹‰çš„å®ä½“çš„åŸå§‹è¯­è¨€è¡¨è¿°ï¼ˆä¿ç•™æ–°é—»ä¸­å®ä½“çš„åŸå§‹è¯­è¨€å½¢å¼ï¼›åŸå§‹è¯­è¨€å®ä½“æ•°ç»„çš„ç´¢å¼•ä¸å®ä½“æ•°ç»„ç´¢å¼•ä¸€ä¸€å¯¹åº”ï¼‰
   - è¯¥äº‹ä»¶çš„æœ¬è´¨æè¿°ï¼ˆä¸€å¥è¯è¯´æ˜â€œè°å¯¹è°åšäº†ä»€ä¹ˆâ€ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡æœ¬ï¼š
{{
  "events": [
    {{
      "abstract": "ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šæ¨è¿Ÿå¯¹VanEckæ¯”ç‰¹å¸ETFç”³è¯·çš„å†³å®š",
      "entities": ["ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼š", "VanEck"],
      "entities_original": ["SEC", "VanEck"],
      "event_summary": "ç›‘ç®¡æœºæ„å»¶é•¿äº†å¯¹æŸèµ„äº§ç®¡ç†å…¬å¸æ¯”ç‰¹å¸ETFç”³è¯·çš„å®¡æŸ¥æœŸ"
    }}
  ]
}}

ã€æ–°é—»ã€‘
æ ‡é¢˜ï¼š{title}
æ­£æ–‡ï¼š{content}"""

    # è°ƒç”¨ API æ± 
    raw_content = API_POOL.call(
        prompt=prompt,
        max_tokens=1500,
        timeout=55,      # é¿å¼€ 60s ä»£ç†è¶…æ—¶
        retries=max_retries
    )

    if not raw_content:
        return []

    # æ¸…ç† Markdown åŒ…è£¹
    try:
        if raw_content.startswith("```json"):
            raw_content = raw_content.split("```json", 1)[1].split("```")[0]
        elif raw_content.startswith("```"):
            raw_content = raw_content.split("```", 1)[1].split("```")[0]

        data = json.loads(raw_content)
        events = data.get("events", [])
        result = []
        for item in events:
            abstract = item.get("abstract", "").strip()
            # ç¡®ä¿entitieså’Œentities_originalä¸€ä¸€å¯¹åº”ï¼Œä¸”éƒ½æœ‰æ•ˆ
            entities_raw = item.get("entities", [])
            entities_original_raw = item.get("entities_original", [])
            entities = []
            entities_original = []
            
            # éå†å¹¶è¿‡æ»¤ï¼Œç¡®ä¿ç´¢å¼•å¯¹åº”
            for ent, ent_original in zip(entities_raw, entities_original_raw):
                if tools.is_valid_entity(ent) and tools.is_valid_entity(ent_original):
                    entities.append(ent)
                    entities_original.append(ent_original)
            summary = item.get("event_summary", "").strip()
            if abstract and entities and summary:
                result.append({
                    "abstract": abstract,
                    "entities": entities,
                    "entities_original": entities_original,
                    "event_summary": summary
                })
        return result
    except Exception as e:
        tools.log(f"[LLMè·å–] âŒ LLM è¿”å›å†…å®¹è§£æå¤±è´¥: {e}")
        return []
    


# ======================
# ä¸»å¤„ç†æµç¨‹
# ======================

def get_unprocessed_news_files() -> List[Path]:
    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r") as f:
            processed_ids = set(line.strip() for line in f if line.strip())
    
    unprocessed = []
    for raw_file in sorted(tools.RAW_NEWS_DIR.glob("*.jsonl")):
        deduped_file = tools.DEDUPED_NEWS_DIR / f"{raw_file.stem}_deduped.jsonl"
        if not deduped_file.exists():
            deduper = NewsDeduplicator(threshold=tools.DEDUPE_THRESHOLD)
            # å…ˆç”¨ processed_ids è¿‡æ»¤â€œå†å²å·²å¤„ç†æ–°é—»â€ï¼Œå†å†™å…¥å»é‡æ–‡ä»¶
            deduper.dedupe_file(raw_file, deduped_file, processed_ids)
        unprocessed.append(deduped_file)
    return unprocessed

def process_news_stream():
    tools.log("ğŸš€ å¯åŠ¨ Agent1ï¼šæµå¼äº‹ä»¶ä¸çœŸå®å®ä½“æå–...")
    files = get_unprocessed_news_files()
    if not files:
        tools.log("ğŸ“­ æ— å¯å¤„ç†æ–°é—»æ–‡ä»¶")
        return

    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r") as f:
            processed_ids = set(line.strip() for line in f if line.strip())

    total_processed = 0
    with open(tools.PROCESSED_IDS_FILE, "a") as id_log:
        for file_path in files:
            tools.log(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news = json.loads(line)
                        raw_id = str(news.get("id", "")).strip()
                        source = news.get("source", "unknown").strip().lower()
                        
                        if not raw_id or not source:
                            tools.log("âš ï¸ è·³è¿‡æ—  ID æˆ–æ—  source çš„æ–°é—»")
                            continue

                        global_id = f"{source}:{raw_id}"  # ğŸ‘ˆ å…³é”®ï¼šå¸¦å‰ç¼€çš„å…¨å±€å”¯ä¸€ ID

                        if global_id in processed_ids:
                            continue

                        title = news.get("title", "")
                        content = news.get("content", "")

                        # ä¸ºé¿å…è¶…é•¿æ­£æ–‡å¯¼è‡´ LLM è¶…å‡º token æˆ–è¿”å›å¼‚å¸¸ JSONï¼Œè¿™é‡Œå¯¹æ­£æ–‡åšé•¿åº¦æˆªæ–­
                        MAX_CONTENT_CHARS = 2000  # å¯è°ƒï¼Œæ¯”å¦‚ 1500 / 3000
                        if isinstance(content, str) and len(content) > MAX_CONTENT_CHARS:
                            content = content[:MAX_CONTENT_CHARS] + "â€¦â€¦ã€åæ–‡å·²æˆªæ–­ã€‘"

                        extracted = llm_extract_events(title, content)

                        if extracted:
                            all_entities = []
                            all_entities_original = []
                            for ev in extracted:
                                all_entities.extend(ev["entities"])
                                all_entities_original.extend(ev["entities_original"])
                            if all_entities and len(all_entities) == len(all_entities_original):
                                # ä¼˜å…ˆä½¿ç”¨æ–°é—»è‡ªèº«çš„æ—¶é—´æˆ³ï¼ˆç”±é‡‡é›†å™¨æä¾›ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
                                ts = news.get("timestamp")
                                # éƒ¨åˆ†æ—§æ•°æ®å¯èƒ½æ˜¯ datetime å¯¹è±¡æˆ–å…¶ä»–ç±»å‹ï¼Œç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²
                                published_at = None
                                if ts:
                                    try:
                                        published_at = (
                                            ts if isinstance(ts, str) else str(ts)
                                        )
                                    except Exception:
                                        published_at = None

                                update_entities(all_entities, all_entities_original, source, published_at)
                                update_abstract_map(extracted, source, published_at)
                                total_processed += 1

                                id_log.write(global_id + "\n")  # ğŸ‘ˆ å†™å…¥å¸¦å‰ç¼€çš„ ID
                                processed_ids.add(global_id)
                            else:
                                tools.log(f"ğŸ” æ–°é—» {global_id}ï¼šLLM è¿”å›äº‹ä»¶ä½†æ— æœ‰æ•ˆå®ä½“ï¼Œæš‚ä¸æ ‡è®°")
                        else:
                            tools.log(f"â³ æ–°é—» {global_id}ï¼šLLM æœªè¿”å›æœ‰æ•ˆäº‹ä»¶ï¼Œä¿ç•™é‡è¯•æœºä¼š")

                    except Exception as e:
                        tools.log(f"âš ï¸ å¤„ç†å•æ¡æ–°é—»å¤±è´¥: {e}")
            
            # å¤„ç†å®Œæ–‡ä»¶ååˆ é™¤å¯¹åº”çš„raw_newsæ–‡ä»¶å’Œè¯¥deduped_newsæ–‡ä»¶
            try:
                # æ‰¾åˆ°å¯¹åº”çš„raw_newsæ–‡ä»¶ï¼ˆå»æ‰"_deduped"åç¼€ï¼‰
                raw_file_name = file_path.stem.replace("_deduped", "") + ".jsonl"
                raw_file_path = tools.RAW_NEWS_DIR / raw_file_name
                
                # åˆ é™¤raw_newsæ–‡ä»¶
                if raw_file_path.exists():
                    raw_file_path.unlink()
                    tools.log(f"ğŸ—‘ï¸ åˆ é™¤åŸå§‹æ–°é—»æ–‡ä»¶: {raw_file_path.name}")
                
                # åˆ é™¤deduped_newsæ–‡ä»¶
                if file_path.exists():
                    file_path.unlink()
                    tools.log(f"ğŸ—‘ï¸ åˆ é™¤å»é‡æ–°é—»æ–‡ä»¶: {file_path.name}")
            except Exception as e:
                tools.log(f"âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")

    tools.log(f"âœ… å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡å«æœ‰æ•ˆå®ä½“çš„æ–°é—»")
    
    # åœ¨æ‰€æœ‰æ–°é—»å¤„ç†å®Œæˆåç»Ÿä¸€åˆ·æ–°çŸ¥è¯†å›¾è°±
    if total_processed > 0:
        try:
            with tools._refresh_lock:
                threading.Thread(target=refresh_graph, daemon=True).start()
                tools.log("ğŸ”„ å·²å¯åŠ¨çŸ¥è¯†å›¾è°±åˆ·æ–°çº¿ç¨‹")
        except Exception as e:
            tools.log(f"âš ï¸ å¯åŠ¨çŸ¥è¯†å›¾è°±åˆ·æ–°å¤±è´¥: {e}")
    else:
        tools.log("ğŸ“­ æœªå¤„ç†ä»»ä½•æ–°é—»ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±åˆ·æ–°")


# ======================
# å…¥å£
# ======================

if __name__ == "__main__":
    process_news_stream()