# src/agents/agent2.py
"""
æ™ºèƒ½ä½“2ï¼šå®ä½“æ‹“å±•æ–°é—»

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»å®ä½“åº“ä¸­è·å–å·²æå–çš„å®ä½“
2. ä½¿ç”¨è¿™äº›å®ä½“ä½œä¸ºå…³é”®è¯æœç´¢ç›¸å…³æ–°é—»
3. å¯¹æœç´¢åˆ°çš„æ–°é—»è¿›è¡Œå¤„ç†ï¼Œæå–æ›´å¤šç›¸å…³å®ä½“å’Œäº‹ä»¶
4. æ›´æ–°å®ä½“åº“å’Œäº‹ä»¶æ˜ å°„
"""

import os
import sys
import json
import asyncio
from pathlib import Path
from typing import List, Dict, Set, Optional
from datetime import datetime, timezone, timedelta
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from dotenv import load_dotenv
from ..utils.tool_function import tools
from ..data.api_client import DataAPIPool
from ..data.news_collector import NewsType
from .agent1 import llm_extract_events, update_entities, update_abstract_map

# åˆå§‹åŒ–å·¥å…·å’Œæ•°æ®APIæ± 
tools = tools()
data_api_pool = DataAPIPool()

async def expand_news_by_entities(entities: List[str], limit_per_entity: int = 10) -> List[Dict]:
    """
    æ ¹æ®å®ä½“åˆ—è¡¨æœç´¢ç›¸å…³æ–°é—»
    
    Args:
        entities: å®ä½“åˆ—è¡¨
        limit_per_entity: æ¯ä¸ªå®ä½“æœç´¢çš„æ–°é—»æ•°é‡é™åˆ¶
        
    Returns:
        æœç´¢åˆ°çš„ç›¸å…³æ–°é—»åˆ—è¡¨
    """
    expanded_news = []
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨
    news_collectors = []
    available_sources = data_api_pool.list_available_sources()
    
    for source_name in available_sources:
        try:
            collector = data_api_pool.get_collector(source_name)
            news_collectors.append(collector)
        except Exception as e:
            tools.log(f"âš ï¸ æ— æ³•åˆ›å»ºæ–°é—»æ”¶é›†å™¨ {source_name}: {e}")
    
    if not news_collectors:
        tools.log("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨")
        return expanded_news
    
    # ä¸ºæ¯ä¸ªå®ä½“æœç´¢ç›¸å…³æ–°é—»
    for entity in entities:
        tools.log(f"ğŸ” ä¸ºå®ä½“ '{entity}' æœç´¢ç›¸å…³æ–°é—»...")
        
        for collector in news_collectors:
            try:
                # ä½¿ç”¨æœç´¢åŠŸèƒ½è·å–ç›¸å…³æ–°é—»
                if hasattr(collector, 'search_news_by_keyword'):
                    news_list = await collector.search_news_by_keyword(
                        keyword=entity,
                        limit=limit_per_entity
                    )
                    
                    # ä¸ºæ¯æ¡æ–°é—»æ·»åŠ å®ä½“æ ‡ç­¾
                    for news in news_list:
                        news['expanded_from_entity'] = entity
                        news['source'] = collector.__class__.__name__.replace('NewsCollector', '').lower()
                        expanded_news.append(news)
                elif hasattr(collector, 'search'):
                    # å…¼å®¹ä¸åŒçš„æœç´¢æ–¹æ³•å
                    news_list = await collector.search(
                        query=entity,
                        limit=limit_per_entity
                    )
                    
                    for news in news_list:
                        news['expanded_from_entity'] = entity
                        news['source'] = collector.__class__.__name__.replace('Collector', '').lower()
                        expanded_news.append(news)
            except Exception as e:
                tools.log(f"âš ï¸ ä» {collector.__class__.__name__} æœç´¢å®ä½“ '{entity}' ç›¸å…³æ–°é—»å¤±è´¥: {e}")
    
    return expanded_news

def get_recent_entities(time_window_hours: int = 24, limit: int = 50) -> List[str]:
    """
    è·å–æœ€è¿‘æ—¶é—´çª—å£å†…çš„å®ä½“åˆ—è¡¨
    
    Args:
        time_window_hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
        limit: è¿”å›çš„å®ä½“æ•°é‡é™åˆ¶
        
    Returns:
        æœ€è¿‘çš„å®ä½“åˆ—è¡¨
    """
    entities = []
    
    if not tools.ENTITIES_FILE.exists():
        tools.log("âš ï¸ å®ä½“åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return entities
    
    # è¯»å–å®ä½“åº“
    with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
        entity_data = json.load(f)
    
    # æ ¹æ® first_seen æ’åºï¼Œè·å–æœ€è¿‘çš„å®ä½“
    sorted_entities = sorted(
        entity_data.items(),
        key=lambda x: x[1].get('first_seen', ''),
        reverse=True
    )
    
    # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„å®ä½“
    now = datetime.now(timezone.utc)
    time_window = timedelta(hours=time_window_hours)
    
    for entity_name, entity_info in sorted_entities:
        first_seen = entity_info.get('first_seen')
        if first_seen:
            try:
                # è§£ææ—¶é—´å­—ç¬¦ä¸²
                if 'T' in first_seen:
                    # ISOæ ¼å¼æ—¶é—´
                    seen_time = datetime.fromisoformat(first_seen.replace('Z', '+00:00'))
                else:
                    # æ™®é€šæ ¼å¼æ—¶é—´
                    seen_time = datetime.strptime(first_seen, '%Y-%m-%d %H:%M:%S')
                    seen_time = seen_time.replace(tzinfo=timezone.utc)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
                if now - seen_time <= time_window:
                    entities.append(entity_name)
                    if len(entities) >= limit:
                        break
            except Exception as e:
                tools.log(f"âš ï¸ è§£æå®ä½“ '{entity_name}' çš„æ—¶é—´æˆ³å¤±è´¥: {e}")
    
    tools.log(f"âœ… è·å–äº† {len(entities)} ä¸ªæœ€è¿‘å®ä½“")
    return entities

async def process_expanded_news(expanded_news: List[Dict]) -> int:
    """
    å¤„ç†æ‹“å±•çš„æ–°é—»ï¼Œæå–å®ä½“å’Œäº‹ä»¶
    
    Args:
        expanded_news: æ‹“å±•çš„æ–°é—»åˆ—è¡¨
        
    Returns:
        å¤„ç†çš„æ–°é—»æ•°é‡
    """
    processed_count = 0
    
    # åˆ›å»ºå»é‡é›†åˆ
    seen_news = set()
    
    for news in expanded_news:
        try:
            # æ£€æŸ¥æ–°é—»æ˜¯å¦å·²å¤„ç†
            news_id = news.get('id')
            source = news.get('source', 'unknown')
            if news_id:
                news_key = f"{source}:{news_id}"
                if news_key in seen_news:
                    continue
                seen_news.add(news_key)
            
            title = news.get('title', '')
            content = news.get('content', '')
            
            if not title:
                continue
            
            # æå–å®ä½“å’Œäº‹ä»¶
            extracted = llm_extract_events(title, content)
            
            if extracted:
                all_entities = []
                for ev in extracted:
                    all_entities.extend(ev['entities'])
                
                if all_entities:
                    # ä¼˜å…ˆä½¿ç”¨æ–°é—»è‡ªèº«çš„æ—¶é—´æˆ³
                    published_at = news.get('datetime')
                    if published_at and isinstance(published_at, datetime):
                        published_at = published_at.isoformat()
                    
                    # æ›´æ–°å®ä½“åº“å’Œäº‹ä»¶æ˜ å°„
                    update_entities(all_entities, source, published_at)
                    update_abstract_map(extracted, source, published_at)
                    processed_count += 1
                    
        except Exception as e:
            tools.log(f"âš ï¸ å¤„ç†æ‹“å±•æ–°é—»å¤±è´¥: {e}")
    
    return processed_count

async def main():
    """
    ä¸»å‡½æ•°
    """
    tools.log("ğŸš€ å¯åŠ¨ Agent2ï¼šå®ä½“æ‹“å±•æ–°é—»...")
    
    # 1. è·å–æœ€è¿‘çš„å®ä½“
    recent_entities = get_recent_entities(time_window_hours=24, limit=50)
    
    if not recent_entities:
        tools.log("ğŸ“­ æ²¡æœ‰å¯ç”¨çš„å®ä½“è¿›è¡Œæ–°é—»æ‹“å±•")
        return
    
    # 2. ä½¿ç”¨å®ä½“æœç´¢ç›¸å…³æ–°é—»
    tools.log(f"ğŸ” å¼€å§‹æœç´¢ {len(recent_entities)} ä¸ªå®ä½“çš„ç›¸å…³æ–°é—»...")
    expanded_news = await expand_news_by_entities(recent_entities, limit_per_entity=5)
    tools.log(f"âœ… å…±æœç´¢åˆ° {len(expanded_news)} æ¡ç›¸å…³æ–°é—»")
    
    # 3. å¤„ç†æœç´¢åˆ°çš„æ–°é—»
    if expanded_news:
        tools.log("ğŸ“„ å¼€å§‹å¤„ç†æ‹“å±•çš„æ–°é—»...")
        processed_count = await process_expanded_news(expanded_news)
        tools.log(f"âœ… æˆåŠŸå¤„ç† {processed_count} æ¡æ‹“å±•æ–°é—»")
    
    tools.log("ğŸ‰ å®ä½“æ‹“å±•æ–°é—»ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
