from __future__ import annotations

"""Generated from `pages/4_Knowledge_Graph.py`.

This module contains the page implementation as a `render()` function.
The wrapper in `pages/` is responsible for `init_page()`.
"""

import streamlit as st
import json
import networkx as nx
import streamlit.components.v1 as components
from datetime import datetime, timedelta, timezone
from collections import OrderedDict
import altair as alt
import pandas as pd

from src.web import utils
from src.web.config import DATA_DIR
from src.web.framework.user_context import can_write, get_user_context, render_user_context_controls
from src.web.services.run_store import cache_dir
import hashlib




def render() -> None:
    render_user_context_controls()
    # --- æ•°æ®åŠ è½½ ---
    kg_file = DATA_DIR / "knowledge_graph.json"
    kg_vis_file = DATA_DIR / "kg_visual.json"
    kg_timeline_file = DATA_DIR / "kg_visual_timeline.json"
    with st.spinner("Loading graph data..."):
        entities = utils.load_entities()
        events = utils.load_events()

        kg_data = {}
        if kg_file.exists():
            try:
                kg_data = json.loads(kg_file.read_text(encoding="utf-8"))
            except Exception as e:
                st.warning(f"çŸ¥è¯†å›¾è°±æ–‡ä»¶è§£æå¤±è´¥ï¼Œå·²å›é€€ï¼š{e}")
                kg_data = {}

        kg_vis_data = {}
        if kg_vis_file.exists():
            try:
                kg_vis_data = json.loads(kg_vis_file.read_text(encoding="utf-8"))
            except Exception as e:
                st.warning(f"å¿«ç…§ kg_visual.json è§£æå¤±è´¥ï¼Œå·²å›é€€åŸå§‹å›¾è°±ï¼š{e}")
                kg_vis_data = {}
        else:
            st.info("æœªæ‰¾åˆ° kg_visual.jsonï¼Œå°†ä½¿ç”¨åŸå§‹çŸ¥è¯†å›¾è°±æ•°æ®ã€‚")

        kg_timeline_data = []
        if kg_timeline_file.exists():
            try:
                kg_timeline_data = json.loads(kg_timeline_file.read_text(encoding="utf-8"))
            except Exception as e:
                st.warning(f"æ—¶é—´çº¿å¿«ç…§ kg_visual_timeline.json è§£æå¤±è´¥ï¼Œå·²å›é€€åŸå§‹äº‹ä»¶ï¼š{e}")
                kg_timeline_data = []
        else:
            st.info("æœªæ‰¾åˆ° kg_visual_timeline.jsonï¼Œå°†ä½¿ç”¨åŸå§‹äº‹ä»¶æ•°æ®ã€‚")

    def parse_dt(val: str):
        if not val:
            return None
        try:
            return datetime.fromisoformat(str(val).replace("Z", "+00:00"))
        except Exception:
            return None

    def within_last_hours(val: str, hours: int) -> bool:
        dt = parse_dt(val)
        if not dt:
            return False
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt >= (datetime.now(timezone.utc) - timedelta(hours=hours))

    # --- ä¾§è¾¹æ æ§åˆ¶ ---
    with st.sidebar:
        mode = st.radio("æ•°æ®æº", ["äº‹ä»¶-å®ä½“æ˜ å°„ (EA)", "å‹ç¼©å›¾è°± (KG)"], index=0)
        display_window = st.radio("å±•ç¤ºæ—¶é—´çª—ï¼ˆä»…å½±å“å±•ç¤º/å®¡æŸ¥ï¼Œä¸å½±å“æŠ“å–ï¼‰", ["æœ€è¿‘24h", "æœ€è¿‘7d", "å…¨éƒ¨"], index=0)
        window_hours = 24 if display_window == "æœ€è¿‘24h" else (24 * 7 if display_window == "æœ€è¿‘7d" else 0)
        seed_strategy = st.selectbox("å­å›¾ç§å­ç­–ç•¥ï¼ˆæ— èšç„¦å®ä½“æ—¶ç”Ÿæ•ˆï¼‰", ["é«˜å…³è”å®ä½“ï¼ˆé»˜è®¤ï¼‰", "æœ€è¿‘äº‹ä»¶"], index=0)
        recent_event_limit = st.slider("æœ€è¿‘äº‹ä»¶æ•°ï¼ˆç§å­=æœ€è¿‘äº‹ä»¶ï¼‰", 10, 300, 80, 10)
        all_entities = list(entities.keys()) if mode == "äº‹ä»¶-å®ä½“æ˜ å°„ (EA)" else list((kg_data.get("entities") or {}).keys())
        placeholder_label = "(All / Top Nodes - EA)" if mode == "äº‹ä»¶-å®ä½“æ˜ å°„ (EA)" else "(All / Top Nodes - KG)"
        # æ”¯æŒä» Data Inspector è·³è½¬èšç„¦å®ä½“
        focus_from_di = st.session_state.pop("kg_focus_entity", None) if "kg_focus_entity" in st.session_state else None
        options = [placeholder_label] + sorted(all_entities)
        default_index = 0
        if focus_from_di and focus_from_di in options:
            default_index = options.index(focus_from_di)

        search_query = st.selectbox(
            "Focus on Entity",
            options=options,
            index=default_index,
            help="Select an entity to view its specific connections."
        )
        hop_depth = st.slider("Hop Depth (èšç„¦æ¨¡å¼)", 1, 4, 1, help="ä»é€‰å®šå®ä½“å‡ºå‘ï¼Œæœ€å¤šæ‹“å±•çš„è¾¹æ•°ï¼ˆå®ä½“-äº‹ä»¶-å®ä½“-...ï¼‰ã€‚")
        # 2. æ˜¾ç¤ºè®¾ç½®
        max_nodes = st.slider("Max Nodesï¼ˆå»ºè®®â‰¤500ï¼Œå¦åˆ™PyViså¯èƒ½å¾ˆæ…¢ï¼‰", 10, 3000, 300, help="Limit total nodes for better performance")
        physics_enabled = st.checkbox("Enable Physics", value=True)
        enable_pyvis = st.checkbox("å¯ç”¨PyViså¤æ‚å›¾è°±ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰", value=True)
        auto_timeline = st.checkbox("æ˜¾ç¤ºèšç„¦å®ä½“æ—¶é—´çº¿", value=True, help="åœ¨ä¸‹æ–¹æ—¶é—´çº¿è§†å›¾ä¸­è‡ªåŠ¨ä½¿ç”¨å½“å‰èšç„¦å®ä½“ï¼ˆKG/EA å‡å¯ï¼‰")
        # æ—¶é—´çº¿å‚æ•°
        entity_opts = sorted(list(entities.keys()))
        default_tl = "(è¯·é€‰æ‹©)"
        if auto_timeline and search_query not in ["(All / Top Nodes - EA)", "(All / Top Nodes - KG)", "(All / Top Nodes)"]:
            default_tl = search_query
        # æ—¶é—´çº¿å®ä½“ç›´æ¥å¤ç”¨å½“å‰èšç„¦å®ä½“ï¼ˆé All/Topï¼‰ï¼Œå¦åˆ™ä¸ºæœªé€‰æ‹©
        timeline_entity = search_query if search_query not in [placeholder_label, "(All / Top Nodes)"] else "(è¯·é€‰æ‹©)"
        limit_events = st.slider("æœ€å¤šæ˜¾ç¤ºäº‹ä»¶æ•°", 10, 500, 200, 10)
        st.divider()
        if mode == "äº‹ä»¶-å®ä½“æ˜ å°„ (EA)":
            st.caption(f"Total Entities: {len(entities)}")
            st.caption(f"Total Events: {len(events)}")
        else:
            if kg_vis_data:
                st.caption(f"KG (vis) Nodes: {len(kg_vis_data.get('nodes') or [])}")
                st.caption(f"KG (vis) Edges: {len(kg_vis_data.get('edges') or [])}")
            else:
                st.caption(f"KG Entities: {len(kg_data.get('entities') or {})}")
                st.caption(f"KG Events: {len(kg_data.get('events') or {})}")

    if mode == "äº‹ä»¶-å®ä½“æ˜ å°„ (EA)":
        if not entities or not events:
            st.warning("çŸ¥è¯†å›¾è°±ä¸ºç©ºã€‚è¯·è¿è¡Œæµæ°´çº¿æ¥å¡«å……æ•°æ®ã€‚")
            st.stop()
    else:
        # KG æ¨¡å¼ä¼˜å…ˆç”¨å¯è§†åŒ–å¿«ç…§
        if kg_vis_data:
            pass
        elif not kg_data or not kg_data.get("entities") or not kg_data.get("events"):
            st.warning("çŸ¥è¯†å›¾è°±(KG)ä¸ºç©ºã€‚")
            st.stop()

    edge_list = []
    event_ids = set()
    if mode == "äº‹ä»¶-å®ä½“æ˜ å°„ (EA)":
        event_ids = {f"EVT:{k}" for k in events.keys()}
        for evt_abstract, evt_data in events.items():
            # å±•ç¤ºæ—¶é—´çª—è¿‡æ»¤ï¼ˆä¼˜å…ˆç”¨ published_atï¼Œå¦åˆ™ first_seenï¼‰
            if window_hours:
                ts = str(evt_data.get("published_at") or evt_data.get("first_seen") or "")
                if not within_last_hours(ts, window_hours):
                    continue
            evt_id = f"EVT:{evt_abstract}"  #ä»¥æ­¤åŒºåˆ†
            evt_summary = evt_data.get('event_summary', evt_abstract)
            for ent in evt_data.get('entities', []):
                if ent in entities:
                    edge_list.append((evt_id, ent, {"title": evt_summary}))
    else:
        # KG æ¨¡å¼ï¼šè‹¥éœ€è¦æ—¶é—´çª—è¿‡æ»¤ï¼Œä¼˜å…ˆä½¿ç”¨ timeline å¿«ç…§æ„å»ºå­å›¾ï¼ˆå¦åˆ™æ— æ³•æŒ‰æ—¶é—´è¿‡æ»¤ï¼‰
        if window_hours and kg_timeline_data:
            for evt in kg_timeline_data:
                t = parse_dt(evt.get("time"))
                if not t:
                    continue
                if t.tzinfo is None:
                    t = t.replace(tzinfo=timezone.utc)
                if t < (datetime.now(timezone.utc) - timedelta(hours=window_hours)):
                    continue
                abstract = evt.get("abstract", "")
                evt_id = f"EVT:{abstract}"
                event_ids.add(evt_id)
                title = evt.get("event_summary", "") or abstract
                for ent in evt.get("entities", []) or []:
                    edge_list.append((evt_id, ent, {"title": title}))
        elif window_hours and (not kg_timeline_data):
            st.info("æç¤ºï¼šå½“å‰é€‰æ‹©äº†æ—¶é—´çª—è¿‡æ»¤ï¼Œä½†æœªæ‰¾åˆ° `kg_visual_timeline.json`ã€‚KGæ¨¡å¼å°†å›é€€ä¸ºä¸æŒ‰æ—¶é—´è¿‡æ»¤çš„å±•ç¤ºã€‚")
        elif kg_vis_data:
            vis_nodes = kg_vis_data.get("nodes", [])
            vis_edges = kg_vis_data.get("edges", [])
            for n in vis_nodes:
                if n.get("type") == "event":
                    event_ids.add(n.get("id"))
            for e in vis_edges:
                u, v = e.get("from"), e.get("to")
                edge_list.append((u, v, {"title": e.get("title", "")}))
        else:
            kg_entities = kg_data.get("entities", {})
            kg_events = kg_data.get("events", {})
            kg_edges = kg_data.get("edges", [])
            event_ids = set(kg_events.keys())
            for e in kg_edges:
                u = e.get("from")
                v = e.get("to")
                if not u or not v:
                    continue
                title = ""
                evt_key = v[4:] if isinstance(v, str) and v.startswith("EVT:") else v
                if evt_key in kg_events:
                    title = kg_events[evt_key].get("event_summary", "") or kg_events[evt_key].get("abstract", "")
                edge_list.append((u, v, {"title": title}))

    # --- è¿‡æ»¤é€»è¾‘ ---
    target_nodes = set()
    from collections import defaultdict, deque
    adj = defaultdict(set)
    for u, v, _ in edge_list:
        adj[u].add(v)
        adj[v].add(u)

    # èŠ‚ç‚¹ç±»å‹åˆ¤æ–­
    def is_event_node(node: str) -> bool:
        if isinstance(node, str) and node.startswith("EVT:"):
            return True
        return node in event_ids

    if search_query != "(All / Top Nodes)" and search_query != "(All / Top Nodes - EA)" and search_query != "(All / Top Nodes - KG)":
        # 1. èšç„¦æ¨¡å¼ï¼šä»é€‰å®šå®ä½“å‡ºå‘ï¼ŒæŒ‰ hop_depth åš BFSï¼ˆå®ä½“-äº‹ä»¶äº¤æ›¿ï¼‰
        target_nodes.add(search_query)
        frontier = {search_query}
        for _ in range(hop_depth):
            next_frontier = set()
            for node in frontier:
                next_frontier |= adj.get(node, set())
            next_frontier -= target_nodes
            target_nodes |= next_frontier
            frontier = next_frontier
    else:
        # 2. å…¨å±€æ¨¡å¼ï¼šæŒ‰åº¦æ•°ï¼ˆè¿æ¥æ•°ï¼‰å– Top N å®ä½“ + ç›¸å…³äº‹ä»¶
        if seed_strategy == "æœ€è¿‘äº‹ä»¶" and window_hours:
            # ä»¥æœ€è¿‘äº‹ä»¶ä¸ºç§å­ï¼šé€‰æ‹©æœ€è¿‘Nä¸ªäº‹ä»¶åŠå…¶ç›¸å…³å®ä½“
            seed_events = []
            if mode == "äº‹ä»¶-å®ä½“æ˜ å°„ (EA)":
                for abstract, info in (events or {}).items():
                    if isinstance(info, dict):
                        ts = str(info.get("published_at") or info.get("first_seen") or "")
                        t = parse_dt(ts)
                        if t and (t.tzinfo is None):
                            t = t.replace(tzinfo=timezone.utc)
                        if t and within_last_hours(ts, window_hours):
                            seed_events.append((t, f"EVT:{abstract}", info.get("entities") or []))
            elif kg_timeline_data:
                for evt in kg_timeline_data:
                    t = parse_dt(evt.get("time"))
                    if not t:
                        continue
                    if t.tzinfo is None:
                        t = t.replace(tzinfo=timezone.utc)
                    if t >= (datetime.now(timezone.utc) - timedelta(hours=window_hours)):
                        abstract = evt.get("abstract", "")
                        seed_events.append((t, f"EVT:{abstract}", evt.get("entities") or []))

            seed_events = sorted(seed_events, key=lambda x: x[0], reverse=True)[:recent_event_limit]
            target_nodes = set()
            for _, evt_id, ents in seed_events:
                target_nodes.add(evt_id)
                for e in ents:
                    target_nodes.add(e)
            # å†åšä¸€æ¬¡ capï¼Œé¿å…æç«¯çˆ†ç‚¸
            if len(target_nodes) > max_nodes:
                target_nodes = set(list(target_nodes)[:max_nodes])
        else:
            # é»˜è®¤ï¼šé«˜å…³è”å®ä½“ï¼ˆåº¦æ•°Topï¼‰
            temp_G = nx.Graph()
            temp_G.add_edges_from([(u, v) for u, v, _ in edge_list])
            degrees = dict(temp_G.degree())
            top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:max_nodes]
            target_nodes = set(top_nodes)

    # --- æ„å»ºæœ€ç»ˆå¯è§†åŒ–å›¾ ---
    visual_G = nx.Graph()

    count = 0
    for u, v, attr in edge_list:
        if u in target_nodes and v in target_nodes:
            # æ·»åŠ èŠ‚ç‚¹ï¼ˆå¦‚æœæœªæ·»åŠ ï¼‰
            if u not in visual_G:
                # åˆ¤æ–­ç±»å‹
                if is_event_node(u):
                    label = u[4:20] + "..." if isinstance(u, str) and u.startswith("EVT:") else str(u)[:20] + "..."
                    visual_G.add_node(u, label=label, title=str(u)[4:] if isinstance(u, str) and u.startswith("EVT:") else str(u), group='Event', color='#ff7f0e', size=15)
                else:
                    visual_G.add_node(u, label=str(u), group='Entity', color='#1f77b4', size=25)
        
            if v not in visual_G:
                if is_event_node(v):
                    label = v[4:20] + "..." if isinstance(v, str) and v.startswith("EVT:") else str(v)[:20] + "..."
                    visual_G.add_node(v, label=label, title=str(v)[4:] if isinstance(v, str) and v.startswith("EVT:") else str(v), group='Event', color='#ff7f0e', size=15)
                else:
                    visual_G.add_node(v, label=str(v), group='Entity', color='#1f77b4', size=25)
        
            visual_G.add_edge(u, v, title=attr.get("title"))
            count += 1
        
    # é¢„è®¡ç®—æ—¶é—´çº¿æ•°æ®ï¼ˆç”¨äº Timeline / Timeline Detailsï¼‰
    rows: list[dict] = []
    co_counter: dict[str, int] = {}
    if timeline_entity and timeline_entity != "(è¯·é€‰æ‹©)":
        # EA æ¨¡å¼ä¼˜å…ˆä½¿ç”¨ events.jsonï¼›KG æ¨¡å¼åœ¨æœ‰ timeline å¿«ç…§æ—¶ä½¿ç”¨ kg_visual_timeline.json
        use_timeline_snapshot = (mode == "å‹ç¼©å›¾è°± (KG)") and bool(kg_timeline_data)

        def _accept_time(t: datetime | None) -> bool:
            if not t:
                return False
            if t.tzinfo is None:
                t = t.replace(tzinfo=timezone.utc)
            if not window_hours:
                return True
            return t >= (datetime.now(timezone.utc) - timedelta(hours=window_hours))

        if use_timeline_snapshot:
            for evt in kg_timeline_data:
                ents = evt.get("entities", []) or []
                if timeline_entity not in ents:
                    continue
                t = parse_dt(evt.get("time"))
                if not _accept_time(t):
                    continue
                co_entities = [e for e in ents if e != timeline_entity]
                for ce in co_entities:
                    co_counter[ce] = co_counter.get(ce, 0) + 1
                rows.append(
                    {
                        "abstract": evt.get("abstract", ""),
                        "event_summary": evt.get("event_summary", ""),
                        "time_dt": t,
                        "co_entities": ", ".join(co_entities[:5]),
                        "co_entities_raw": co_entities,
                    }
                )
        else:
            for abstract, evt in (events or {}).items():
                ents = evt.get("entities", []) or []
                if timeline_entity not in ents:
                    continue
                t = parse_dt(evt.get("published_at") or evt.get("first_seen"))
                if not _accept_time(t):
                    continue
                co_entities = [e for e in ents if e != timeline_entity]
                for ce in co_entities:
                    co_counter[ce] = co_counter.get(ce, 0) + 1
                rows.append(
                    {
                        "abstract": abstract,
                        "event_summary": evt.get("event_summary", "") or abstract,
                        "time_dt": t,
                        "co_entities": ", ".join(co_entities[:5]),
                        "co_entities_raw": co_entities,
                    }
                )

        # å–â€œæœ€è¿‘ N æ¡â€ï¼Œä½†ä¿æŒæ—¶é—´å‡åºï¼ˆä¾¿äºæ—¶é—´çº¿/å›¾è¡¨ï¼‰
        rows = [r for r in rows if r.get("time_dt")]
        rows_sorted_desc = sorted(rows, key=lambda x: x["time_dt"], reverse=True)
        rows_top = rows_sorted_desc[:limit_events]
        rows = sorted(rows_top, key=lambda x: x["time_dt"])


    # --- ç¤¾åŒº/ä¸»é¢˜æ‘˜è¦ï¼ˆè½»é‡ï¼‰ ---
    community_rows = []
    try:
        if visual_G.number_of_nodes() <= 800 and visual_G.number_of_edges() > 0:
            from networkx.algorithms.community import greedy_modularity_communities

            comms = list(greedy_modularity_communities(visual_G))
            deg = dict(visual_G.degree())
            for i, cset in enumerate(comms[:12]):
                nodes = list(cset)
                top = sorted(nodes, key=lambda n: deg.get(n, 0), reverse=True)[:6]
                community_rows.append(
                    {
                        "community": i + 1,
                        "size": len(nodes),
                        "top_nodes": ", ".join([str(t) for t in top]),
                    }
                )
    except Exception:
        community_rows = []

    KG, EntityDetails, Timeline, Community, TimelineDetails = st.tabs(["KG", "Entity Details", "Timeline", "ç¤¾åŒº/ä¸»é¢˜", "Timeline Details"])

    with KG:
        st.subheader("ğŸ•¸ï¸ å›¾è°±è§†å›¾ï¼ˆPyVisï¼‰")
        st.caption("ä¸ºé¿å…æ¯æ¬¡äº¤äº’éƒ½é‡ç®—å¯¼è‡´åŠ è½½å¾ˆæ…¢ï¼šåªæœ‰ç‚¹å‡»â€œç”Ÿæˆ/åˆ·æ–°å›¾è°±â€æ‰ä¼šç”ŸæˆPyVisï¼›å¦åˆ™å±•ç¤ºè½»é‡ä¿¡æ¯ã€‚")

        with st.expander("å¯¼å‡ºï¼ˆå½“å‰å­å›¾ï¼‰", expanded=False):
            try:
                # å¯¼å‡ºå½“å‰å­å›¾ï¼ˆèŠ‚ç‚¹/è¾¹ï¼‰ä¸º JSONï¼Œä¾¿äºå­¦æœ¯/å•†ä¸šåœºæ™¯è¿›ä¸€æ­¥å¤„ç†
                nodes_payload = list(visual_G.nodes(data=True))
                edges_payload = list(visual_G.edges(data=True))
                export_obj = {"nodes": nodes_payload, "edges": edges_payload, "meta": {"window": display_window, "mode": mode}}
                st.download_button(
                    "ä¸‹è½½å½“å‰å­å›¾ JSON",
                    data=json.dumps(export_obj, ensure_ascii=False, indent=2),
                    file_name=f"subgraph_{mode}_{display_window}.json",
                    mime="application/json",
                    use_container_width=True,
                )
            except Exception as e:
                st.warning(f"å¯¼å‡ºå¤±è´¥ï¼š{e}")

            # æä¾›åŸå§‹æ–‡ä»¶å¯¼å‡ºå…¥å£
            if not can_write():
                st.info("viewer è§’è‰²é»˜è®¤ä¸æä¾›åŸå§‹æ–‡ä»¶ä¸‹è½½ï¼ˆæƒé™å ä½ï¼‰ã€‚")
            else:
                for p in [kg_file, kg_vis_file, kg_timeline_file]:
                    if p.exists():
                        st.download_button(
                            f"ä¸‹è½½åŸå§‹æ–‡ä»¶ï¼š{p.name}",
                            data=p.read_bytes(),
                            file_name=p.name,
                            use_container_width=True,
                        )

        # å‚æ•°ç­¾åç”¨äºç¼“å­˜
        cache_key = f"{mode}|{display_window}|{search_query}|{hop_depth}|{max_nodes}|{physics_enabled}|{len(edge_list)}"
        if "kg_pyvis_cache" not in st.session_state:
            st.session_state.kg_pyvis_cache = OrderedDict()
        if not isinstance(st.session_state.kg_pyvis_cache, OrderedDict):
            st.session_state.kg_pyvis_cache = OrderedDict(st.session_state.kg_pyvis_cache)

        project_id = get_user_context().project_id
        use_disk_cache = st.checkbox(
            "è·¨ä¼šè¯ç£ç›˜ç¼“å­˜ï¼ˆæ›´å¿«ï¼‰",
            value=True,
            help="æŠŠç”Ÿæˆçš„ PyVis HTML ç¼“å­˜åˆ° data/projects/<project_id>/cache/pyvis/ï¼Œä¸‹æ¬¡ç§’å¼€ã€‚",
        )
        disk_dir = cache_dir(project_id) / "pyvis"
        disk_dir.mkdir(parents=True, exist_ok=True)
        key_hash = hashlib.sha1(cache_key.encode("utf-8")).hexdigest()[:16]
        disk_path = disk_dir / f"pyvis_{key_hash}.html"

        col_btn, col_info = st.columns([1, 2])
        with col_btn:
            gen = st.button("ç”Ÿæˆ/åˆ·æ–°å›¾è°±", type="primary", use_container_width=True, disabled=(not enable_pyvis))
            if st.button("æ¸…ç†PyVisç¼“å­˜", use_container_width=True):
                st.session_state.kg_pyvis_cache = OrderedDict()
                st.success("å·²æ¸…ç†ç¼“å­˜")
        with col_info:
            st.info(f"å½“å‰å­å›¾ï¼šNodes={visual_G.number_of_nodes()} / Edges={visual_G.number_of_edges()}ï¼ˆ{display_window}ï¼‰")

        if not enable_pyvis:
            st.warning("å·²å…³é—­PyViså¤æ‚å›¾è°±ã€‚ä½ ä»å¯åœ¨ä¸‹æ–¹æŸ¥çœ‹å®ä½“è¯¦æƒ…ä¸æ—¶é—´çº¿ã€‚")
        else:
            html_string = st.session_state.kg_pyvis_cache.get(cache_key)
            if (html_string is None) and use_disk_cache and disk_path.exists() and (not gen):
                try:
                    html_string = disk_path.read_text(encoding="utf-8")
                    st.session_state.kg_pyvis_cache[cache_key] = html_string
                    st.session_state.kg_pyvis_cache.move_to_end(cache_key)
                except Exception:
                    html_string = None
            if gen or (html_string is None):
                try:
                    from pyvis.network import Network
                    import tempfile

                    with st.spinner("Generating PyVis graph (may take time)..."):
                        net = Network(height="700px", width="100%", bgcolor="#ffffff", font_color="black")
                        net.from_nx(visual_G)
                        if physics_enabled:
                            net.force_atlas_2based()
                        else:
                            net.toggle_physics(False)

                        with tempfile.NamedTemporaryFile(delete=False, suffix=".html") as tmp:
                            net.save_graph(tmp.name)
                            with open(tmp.name, "r", encoding="utf-8") as f:
                                html_string = f.read()

                    # ç¼“å­˜å¹¶é™åˆ¶å¤§å°ï¼Œé¿å… session_state æ— é™å¢é•¿
                    st.session_state.kg_pyvis_cache[cache_key] = html_string
                    st.session_state.kg_pyvis_cache.move_to_end(cache_key)
                    while len(st.session_state.kg_pyvis_cache) > 5:
                        st.session_state.kg_pyvis_cache.popitem(last=False)
                    if use_disk_cache and html_string:
                        try:
                            disk_path.write_text(html_string, encoding="utf-8")
                        except Exception:
                            pass
                except ImportError:
                    st.error("PyVis not installed. Run `pip install pyvis` to view the graph.")
                    html_string = None

            if html_string:
                components.html(html_string, height=710, scrolling=False)

    with Community:
        st.subheader("ğŸ§© ç¤¾åŒº/ä¸»é¢˜æ‘˜è¦ï¼ˆè½»é‡ï¼‰")
        st.caption("å¯¹å½“å‰å­å›¾åšç¤¾åŒºåˆ’åˆ†ï¼ˆèŠ‚ç‚¹æ•°è¿‡å¤§æ—¶è‡ªåŠ¨è·³è¿‡ï¼‰ã€‚åç»­å¯æ‰©å±•ä¸ºâ€œä¸»é¢˜æ ‡ç­¾/æ‘˜è¦/æ¨é€ä¿¡å·â€ã€‚")
        if community_rows:
            st.dataframe(pd.DataFrame(community_rows), hide_index=True, use_container_width=True)
        else:
            st.info("å½“å‰å­å›¾è§„æ¨¡è¾ƒå¤§æˆ–ç¼ºå°‘è¾¹ï¼Œå·²è·³è¿‡ç¤¾åŒºåˆ’åˆ†ã€‚")

    # --- èŠ‚ç‚¹è¯¦æƒ…é¢æ¿ ---
    with EntityDetails:
        if search_query != "(All / Top Nodes)":
            st.divider()
            st.subheader(f"ğŸ“˜ Entity Details: {search_query}")
        
            ent_info = entities.get(search_query, {})
            c1, c2 = st.columns(2)
            with c1:
                st.write("**Sources:**", ", ".join(ent_info.get("sources", [])))
                st.write("**First Seen:**", ent_info.get("first_seen", "N/A"))
            with c2:
                st.write("**Aliases/Forms:**", ", ".join(ent_info.get("original_forms", [])))
            
            st.write("**Related Events:**")
            # æŸ¥æ‰¾å…³è”äº‹ä»¶æ‘˜è¦
            related_evts = []
            for evt_abstract, evt_data in events.items():
                if search_query in evt_data.get('entities', []):
                    related_evts.append(evt_data.get('event_summary') or evt_abstract)
                
            for evt in related_evts[:10]:
                st.text(f"â€¢ {evt}")
            if len(related_evts) > 10:
                st.caption(f"... and {len(related_evts)-10} more.")

    with Timeline:
        if timeline_entity and timeline_entity != "(è¯·é€‰æ‹©)" and rows:
            try:
                from pyvis.network import Network
                from pathlib import Path
                import tempfile

                net = Network(
                    height="750px",
                    width="100%",
                    bgcolor="#ffffff",
                    font_color="#333333",
                    directed=True,
                    notebook=False
                )

                # ä½¿ç”¨è‡ªå®šä¹‰ physics å‚æ•°ï¼ˆå…³é”®ï¼šå…³é—­ centralGravityï¼Œå¦åˆ™èŠ‚ç‚¹ä¼šæ•´ä½“å‘ç”»å¸ƒä¸­å¿ƒèšé›†ï¼‰
                net.set_options("""
                {
                "physics": {
                    "enabled": true,
                    "forceAtlas2Based": {
                    "gravitationalConstant": -50,
                    "centralGravity": 0.0,
                    "springLength": 200,
                    "springStrength": 0.08,
                    "damping": 0.8,
                    "avoidOverlap": 1
                    },
                    "maxVelocity": 50,
                    "minVelocity": 10,
                    "solver": "forceAtlas2Based",
                    "timestep": 0.5,
                    "stabilization": {
                    "enabled": true,
                    "iterations": 200,
                    "updateInterval": 25
                    }
                },
                "nodes": {
                    "font": {
                    "size": 16,
                    "face": "arial"
                    }
                },
                "edges": {
                    "arrows": {
                    "to": {
                        "enabled": true,
                        "scaleFactor": 0.5
                    }
                    },
                    "smooth": false,
                    "color": "#999999"
                }
                }
                """)

                # 1. å…ˆæ·»åŠ æ‰€æœ‰å®ä½“èŠ‚ç‚¹ï¼ˆä¸å›ºå®šä½ç½®ï¼‰
                all_entities = set()
                for r in rows:
                    for ce in r.get("co_entities_raw", [])[:8]:  # é™åˆ¶ä¸€ä¸‹æ•°é‡é˜²çˆ†ç‚¸
                        all_entities.add(ce)

                for ent in all_entities:
                    net.add_node(
                        f"ent_{ent}",
                        label=ent,
                        color="#1f77b4",
                        size=20,
                        shape="dot",
                        font={"color": "white", "size": 14},
                        title=ent,
                        mass=1
                    )

                # 2. æ·»åŠ äº‹ä»¶èŠ‚ç‚¹ï¼šå›ºå®š x/y
                for idx, r in enumerate(rows):
                    x = idx * 230
                    ys = [0,60,-60]
                    y = ys[idx%3]
                
                    size = 30 + len(r.get("co_entities_raw", [])) * 3
                    label = r.get("event_summary", "")[:50] + "..." if len(r.get("event_summary", "")) > 50 else r.get("event_summary", "")

                    net.add_node(
                        f"evt_{idx}",
                        label=label,
                        title=r.get("event_summary", ""),
                        x=x,
                        y=y,
                        fixed={"x": True, "y": True},   # å›ºå®šäº‹ä»¶èŠ‚ç‚¹ä½ç½®ï¼
                        # å…³é”®ï¼šäº‹ä»¶èŠ‚ç‚¹ä½ç½®å›ºå®šï¼Œä½†ä»å‚ä¸ç‰©ç† => ä½œä¸ºâ€œé”šç‚¹â€æŠŠç›¸å…³å®ä½“å¸é™„åˆ°å‘¨å›´
                        # å¦‚æœè®¾ä¸º physics=Falseï¼Œä¼šå¯¼è‡´å®ä½“èŠ‚ç‚¹éš¾ä»¥å›´ç»•äº‹ä»¶å½¢æˆç¨³å®šç°‡
                        physics=True,
                        color="#ff7f0e",
                        size=size,
                        shape="dot",
                        font={"size": 18, "color": "white"},
                        shadow=True,
                        mass=5
                    )

                    # æ·»åŠ è¾¹ï¼šå®ä½“ â†’ äº‹ä»¶ï¼ˆç®­å¤´æŒ‡å‘äº‹ä»¶ï¼‰
                    for ce in r.get("co_entities_raw", [])[:8]:
                        # length è¶ŠçŸ­ï¼Œå®ä½“è¶Šè´´è¿‘å…³è”äº‹ä»¶èŠ‚ç‚¹
                        net.add_edge(f"ent_{ce}", f"evt_{idx}", color="#aaaaaa", width=1.5, length=120)

                # å¯é€‰ï¼šåŠ ä¸€ä¸ªéšè—çš„â€œæ—¶é—´ä¸»çº¿â€è®©äº‹ä»¶ä¹‹é—´ä¹Ÿæœ‰è¿çº¿ï¼ˆæ›´æ¸…æ™°ï¼‰
                for i in range(len(rows)-1):
                    net.add_edge(f"evt_{i}", f"evt_{i+1}", color="#ff7f0e", width=3, dashes=True)

                # ä¿å­˜å¹¶æ˜¾ç¤º
                with tempfile.NamedTemporaryFile(delete=False, suffix=".html") as tmp:
                    net.save_graph(tmp.name)
                    html_string = Path(tmp.name).read_text(encoding="utf-8")

                st.components.v1.html(html_string, height=800, scrolling=True)

            except ImportError:
                st.warning("è¯·å…ˆå®‰è£… pyvisï¼š`pip install pyvis`")


    with TimelineDetails:
        st.subheader("æ—¶é—´çº¿è¯¦æƒ…")
        if timeline_entity and timeline_entity != "(è¯·é€‰æ‹©)":
            if rows:
                df_tl = pd.DataFrame(rows)
                chart = alt.Chart(df_tl).mark_line(point=True).encode(
                    x="time_dt:T",
                    y=alt.value(0),
                    tooltip=["time_dt:T", "event_summary:N", "co_entities:N"]
                ).properties(height=120, width="container")
                st.altair_chart(chart, use_container_width=True)
                st.dataframe(df_tl[["time_dt", "event_summary", "co_entities"]], hide_index=True, use_container_width=True)
            
                if co_counter:
                    top_co = sorted(co_counter.items(), key=lambda x: x[1], reverse=True)[:10]
                    st.caption("Top å…±ç°å®ä½“")
                    st.table({"entity": [x[0] for x in top_co], "count": [x[1] for x in top_co]})
            else:
                st.info("è¯¥å®ä½“æ²¡æœ‰å¯å±•ç¤ºçš„å¸¦æ—¶é—´äº‹ä»¶ã€‚")
        else:
            st.info("è¯·é€‰æ‹©ä¸€ä¸ªå®ä½“æŸ¥çœ‹æ—¶é—´çº¿ã€‚")





