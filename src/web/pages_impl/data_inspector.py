from __future__ import annotations

import json
from datetime import datetime, timedelta, timezone

import pandas as pd
import streamlit as st

from src.web import utils
from src.web.config import PROJECT_ROOT
from src.web.framework.user_context import can_write, get_user_context, render_user_context_controls
from src.web.services.pipeline_runner import get_global_pipeline_runner, append_history
from src.web.services.run_store import (
    list_runs,
    load_run_change_pack,
    load_run_context_snapshot,
    save_evidence_content_snippet,
    save_evidence_note,
)
from src.web.services.news_lookup import find_news_by_id
import hashlib


def render() -> None:
    render_user_context_controls()
    st.title("ğŸ•µï¸ Data Inspector")
    st.caption("ç”¨äºå®¡æŸ¥ä¸æ£€ç´¢ï¼šé»˜è®¤èšç„¦æœ€è¿‘24hæ–°å¢ï¼ˆä»¥å‘å¸ƒæ—¶é—´/å‘ç°æ—¶é—´ä¸ºå‡†ï¼‰")

    def normalize_mixed(val):
        if val is None:
            return ""
        if isinstance(val, (list, dict)):
            try:
                return json.dumps(val, ensure_ascii=False)
            except Exception:
                return str(val)
        return str(val)

    tab_recent, tab_runs, tab_entities, tab_events, tab_news, tab_tmp = st.tabs(
        ["ğŸ†• æœ€è¿‘24hæ–°å¢", "ğŸ—‚ï¸ Runs å®¡æŸ¥", "ğŸ§  Entities", "ğŸ”— Events", "ğŸ“° Raw News", "ğŸ—ƒï¸ Extracted Snapshots"]
    )

    def _parse_iso(dt_str: str):
        if not dt_str:
            return None
        try:
            # normalize Z
            return datetime.fromisoformat(dt_str.replace("Z", "+00:00"))
        except Exception:
            return None

    def _within_last(dt_str: str, hours: int) -> bool:
        dt = _parse_iso(dt_str)
        if not dt:
            return False
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt >= (datetime.now(timezone.utc) - timedelta(hours=hours))

    with tab_recent:
        st.subheader("ğŸ†• æœ€è¿‘24hæ–°å¢ï¼ˆå®¡æŸ¥å…¥å£ï¼‰")
        st.caption("é»˜è®¤æŒ‰ published_atï¼ˆè‹¥å­˜åœ¨ï¼‰å¦åˆ™æŒ‰ first_seen è¿‡æ»¤ã€‚ç”¨äºå¿«é€Ÿç¡®è®¤æ–°å¢ä¸é—æ¼ã€‚")

        # 1) æ–°å¢äº‹ä»¶ï¼ˆä¼˜å…ˆï¼‰
        events_data = utils.load_events()
        recent_events = []
        for abstract, info in (events_data or {}).items():
            if not isinstance(info, dict):
                continue
            ts = info.get("published_at") or info.get("first_seen") or ""
            if _within_last(str(ts), 24):
                recent_events.append(
                    {
                        "abstract": abstract,
                        "event_summary": info.get("event_summary", "") or abstract,
                        "time": ts,
                        "entities": normalize_mixed(info.get("entities")),
                    }
                )
        df_recent_evt = pd.DataFrame(recent_events)
        c1, c2 = st.columns([3, 1])
        with c2:
            st.metric("æœ€è¿‘24häº‹ä»¶", len(df_recent_evt))
        with c1:
            if not df_recent_evt.empty:
                st.dataframe(
                    df_recent_evt.sort_values("time", ascending=False),
                    use_container_width=True,
                    hide_index=True,
                )
            else:
                st.info("æœ€è¿‘24hæœªæ£€æµ‹åˆ°æ–°å¢äº‹ä»¶ï¼ˆæˆ–äº‹ä»¶ç¼ºå°‘æ—¶é—´æˆ³å­—æ®µï¼‰ã€‚")

        st.divider()

        # 2) æ–°å¢å®ä½“
        entities_data = utils.load_entities()
        recent_entities = []
        for name, info in (entities_data or {}).items():
            if isinstance(info, dict):
                ts = info.get("first_seen") or ""
            else:
                ts = ""
            if _within_last(str(ts), 24):
                recent_entities.append(
                    {
                        "entity": str(name),
                        "first_seen": ts,
                        "count": (info.get("count") if isinstance(info, dict) else None),
                        "sources": normalize_mixed(info.get("sources") if isinstance(info, dict) else None),
                    }
                )
        df_recent_ent = pd.DataFrame(recent_entities)

        c3, c4 = st.columns([3, 1])
        with c4:
            st.metric("æœ€è¿‘24hå®ä½“", len(df_recent_ent))
        with c3:
            if not df_recent_ent.empty:
                st.dataframe(
                    df_recent_ent.sort_values("first_seen", ascending=False),
                    use_container_width=True,
                    hide_index=True,
                )
            else:
                st.info("æœ€è¿‘24hæœªæ£€æµ‹åˆ°æ–°å¢å®ä½“ï¼ˆæˆ–å®ä½“ç¼ºå°‘ first_seen å­—æ®µï¼‰ã€‚")

        st.divider()

        # 3) è·³è½¬åˆ°å›¾è°±èšç„¦ï¼ˆç”¨ session_state ä¼ é€’ï¼‰
        st.subheader("ğŸ” åœ¨å›¾è°±ä¸­èšç„¦å®ä½“")
        all_names = sorted(list((entities_data or {}).keys()))
        focus = st.selectbox("é€‰æ‹©å®ä½“", options=["(è¯·é€‰æ‹©)"] + all_names, index=0)
        if st.button("åœ¨çŸ¥è¯†å›¾è°±ä¸­èšç„¦", type="primary", use_container_width=True, disabled=(focus == "(è¯·é€‰æ‹©)")):
            st.session_state["kg_focus_entity"] = focus
            st.switch_page("pages/4_Knowledge_Graph.py")

    with tab_runs:
        st.subheader("ğŸ—‚ï¸ æŒ‰ run_id å®¡æŸ¥ï¼ˆæ–°å¢å®ä½“/äº‹ä»¶ + è¯æ®Pinå ä½ï¼‰")
        st.caption("æ¯æ¬¡åœ¨ Pipeline é¡µé¢è¿è¡Œåï¼Œä¼šåœ¨ data/runs ä¸‹ç”Ÿæˆä¸€ä¸ª run å˜æ›´åŒ…ï¼ˆæ–°å¢å®ä½“/äº‹ä»¶ï¼‰ã€‚")

        project_id = get_user_context().project_id
        run_files = list_runs(project_id=project_id, limit=50)
        if not run_files:
            st.info("æš‚æ— è¿è¡Œè®°å½•ã€‚è¯·å…ˆåœ¨ Pipeline é¡µé¢æ‰§è¡Œä¸€æ¬¡è¿è¡Œã€‚")
        else:
            opts = [p.name for p in run_files]
            default = 0
            sel = st.selectbox("é€‰æ‹©è¿è¡Œè®°å½•", options=opts, index=default)
            run_path = next(p for p in run_files if p.name == sel)
            pack = load_run_change_pack(run_path)
            run_id = str(pack.get("run_id") or "")

            st.markdown(f"**run_id**: `{run_id}`")
            st.markdown(f"**pipeline**: {pack.get('pipeline_name')}")
            st.markdown(f"**status**: {pack.get('status')}")
            if pack.get("error"):
                st.error(f"error: {pack.get('error')}")
            if pack.get("completed_steps") and pack.get("total_steps"):
                st.caption(f"steps: {pack.get('completed_steps')}/{pack.get('total_steps')}")

            new_events = pack.get("new_events") or []
            new_entities = pack.get("new_entities") or []
            dup_events = pack.get("duplicate_events") or []
            evidence_rows = pack.get("evidence_events") or []

            c1, c2 = st.columns(2)
            c1.metric("æ–°å¢äº‹ä»¶", len(new_events))
            c2.metric("æ–°å¢å®ä½“", len(new_entities))
            if dup_events:
                st.warning(f"æ£€æµ‹åˆ°å¯èƒ½é‡å¤äº‹ä»¶ï¼ˆabstract å·²å­˜åœ¨ï¼‰ï¼š{len(dup_events)}")

            st.divider()
            st.subheader("æ–°å¢å®ä½“ï¼ˆå¯è·³è½¬KGèšç„¦ï¼‰")
            if new_entities:
                ent_pick = st.selectbox("é€‰æ‹©å®ä½“è·³è½¬", options=["(è¯·é€‰æ‹©)"] + new_entities, index=0)
                if st.button("è·³è½¬åˆ°KGå¹¶èšç„¦è¯¥å®ä½“", type="primary", use_container_width=True, disabled=(ent_pick == "(è¯·é€‰æ‹©)")):
                    st.session_state["kg_focus_entity"] = ent_pick
                    st.switch_page("pages/4_Knowledge_Graph.py")
            else:
                st.info("è¯¥ run æ²¡æœ‰æ–°å¢å®ä½“ã€‚")

            st.divider()
            st.subheader("æ–°å¢äº‹ä»¶ï¼ˆè¯æ®é“¾ + Pinï¼‰")
            if not new_events:
                st.info("è¯¥ run æ²¡æœ‰æ–°å¢äº‹ä»¶ã€‚")
            else:
                evt_pick = st.selectbox("é€‰æ‹©äº‹ä»¶", options=["(è¯·é€‰æ‹©)"] + new_events, index=0)
                if evt_pick != "(è¯·é€‰æ‹©)":
                    if evidence_rows:
                        # è§£ææ¯ä¸€è¡Œæ•°æ®ï¼Œå¤„ç†åˆ¶è¡¨ç¬¦åˆ†éš”çš„å­—ç¬¦ä¸²
                        parsed_candidates = []
                        for r in evidence_rows:
                            if isinstance(r, dict):
                                # å·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥æ·»åŠ 
                                parsed_candidates.append(r)
                            elif isinstance(r, str):
                                # æŒ‰åˆ¶è¡¨ç¬¦åˆ†å‰²
                                parts = r.split('\t')
                                if len(parts) >= 4:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å­—æ®µ
                                    json_str = parts[3].strip()  # ç¬¬4ä¸ªå­—æ®µæ˜¯JSON
                                    # æ¸…ç†å¯èƒ½çš„è½¬ä¹‰å­—ç¬¦
                                    if json_str.startswith('"{') and json_str.endswith('}"'):
                                        json_str = json_str[1:-1]  # å»æ‰å¤–å±‚çš„åŒå¼•å·
                                    json_str = json_str.replace('\\"', '"')  # å¤„ç†è½¬ä¹‰çš„åŒå¼•å·
                                    
                                    try:
                                        parsed = json.loads(json_str)
                                        if isinstance(parsed, dict):
                                            parsed_candidates.append(parsed)
                                    except json.JSONDecodeError:
                                        continue  # è§£æå¤±è´¥åˆ™è·³è¿‡

                        # è¿‡æ»¤å‡ºabstractåŒ¹é…çš„äº‹ä»¶
                        candidates = [r for r in parsed_candidates if isinstance(r, dict) and r.get("abstract") == evt_pick]
                        if candidates:
                            st.caption("è¯æ®é“¾ï¼ˆæ¥è‡ª extracted_events è¾“å‡º + raw_news best-effortï¼‰")
                            st.dataframe(pd.DataFrame(candidates), hide_index=True, use_container_width=True)
                        else:
                            st.info("è¯¥äº‹ä»¶æœªåœ¨æœ¬æ¬¡ extracted_events è¾“å‡ºä¸­æ‰¾åˆ°è¯æ®é“¾ï¼ˆå¯èƒ½æ¥è‡ªå…¶ä»–æ¥æº/åˆå¹¶é€»è¾‘ï¼‰ã€‚")

                    note = st.text_area("å¤‡æ³¨ï¼ˆPinï¼‰", height=120, key=f"pin_note_{run_id}_{evt_pick[:20]}")
                    pin_disabled = (not can_write()) or (not run_id)
                    if st.button("ğŸ“Œ Pin å¤‡æ³¨", use_container_width=True, disabled=pin_disabled):
                        p = save_evidence_note(
                            project_id=project_id,
                            run_id=run_id or "unknown",
                            kind="event",
                            key=evt_pick,
                            note=note,
                            meta={"source": "data_inspector_runs"},
                        )
                        st.success(f"å·²ä¿å­˜ï¼š{p.name}")

                    st.divider()
                    st.subheader("ä¿å­˜åŸæ–‡ç‰‡æ®µï¼ˆä½å­˜å‚¨ï¼Œå ä½å®ç°ï¼‰")
                    if candidates:
                        news_id = str(candidates[0].get("news_id") or "")
                        item = find_news_by_id(news_id) if news_id else None
                        if item and item.content:
                            max_len = st.slider("æˆªæ–­é•¿åº¦", 300, 4000, 1500, 100)
                            snippet = (item.content or "")[: int(max_len)]
                            h = hashlib.sha256((item.content or "").encode("utf-8")).hexdigest()[:24]
                            st.text_area("é¢„è§ˆç‰‡æ®µï¼ˆå°†ä¿å­˜ï¼‰", value=snippet, height=160)
                            if st.button("ğŸ’¾ ä¿å­˜åŸæ–‡ç‰‡æ®µï¼ˆPinï¼‰", use_container_width=True, disabled=pin_disabled):
                                p = save_evidence_content_snippet(
                                    project_id,
                                    run_id=run_id,
                                    news_id=item.news_id,
                                    url=item.url,
                                    title=item.title,
                                    published_at=item.published_at,
                                    source=item.source,
                                    content_snippet=snippet,
                                    content_hash=h,
                                )
                                st.success(f"å·²ä¿å­˜ï¼š{p.name}")
                        else:
                            st.info("æœªèƒ½ä» tmp/raw_news æ‰¾åˆ°è¯¥ news_id çš„ contentï¼ˆå¯èƒ½è¢«æ¸…ç†æˆ–ä¸åœ¨æœ¬åœ°ï¼‰ã€‚")

            st.divider()
            st.subheader("å¤è·‘/æ¢å¤ï¼ˆå ä½å®ç°ï¼‰")
            if not can_write():
                st.info("viewer è§’è‰²ä¸å¯å¤è·‘ã€‚")
            else:
                ctx_file = run_path.parent / f"run_{run_id}_context.json" if run_id else None
                pipeline_def = pack.get("pipeline_def") if isinstance(pack.get("pipeline_def"), dict) else None
                if (not pipeline_def) or (not run_id):
                    st.info("è¯¥ run ç¼ºå°‘ pipeline_def æˆ– run_idï¼Œæ— æ³•å¤è·‘ã€‚")
                else:
                    runner = get_global_pipeline_runner()
                    colA, colB = st.columns(2)
                    with colA:
                        if st.button("ä»å¤´å¤è·‘", type="primary", use_container_width=True, disabled=runner.is_running):
                            history_idx = append_history(pipeline_def)
                            runner.start(pipeline_def, history_idx=history_idx)
                            st.rerun()
                    with colB:
                        start_at = st.number_input("ä»ç¬¬Næ­¥å¼€å§‹ï¼ˆå®éªŒï¼‰", min_value=0, max_value=max(0, len(pipeline_def.get("steps", [])) - 1), value=0, step=1)
                        if st.button("å°è¯•æ¢å¤è¿è¡Œ", use_container_width=True, disabled=runner.is_running):
                            init_data = {}
                            try:
                                if ctx_file and ctx_file.exists():
                                    init_data = load_run_context_snapshot(ctx_file)
                            except Exception:
                                init_data = {}
                            history_idx = append_history(pipeline_def)
                            runner.start(pipeline_def, history_idx=history_idx, start_at=int(start_at), initial_data=init_data)
                            st.rerun()

    with tab_entities:
        col_filter, col_stat = st.columns([3, 1])
        with col_filter:
            entity_search = st.text_input("ğŸ” Search Entities", placeholder="e.g. Bitcoin, SEC...")
        entities_data = utils.load_entities()

        if entities_data:
            df_ent = pd.DataFrame.from_dict(entities_data, orient="index")
            df_ent.reset_index(inplace=True)
            df_ent.rename(columns={"index": "Entity Name"}, inplace=True)
            if "sources" in df_ent.columns:
                df_ent["sources"] = df_ent["sources"].apply(normalize_mixed)

            if entity_search:
                df_ent = df_ent[df_ent["Entity Name"].str.contains(entity_search, case=False, na=False)]

            with col_stat:
                st.metric("Total Entities", len(df_ent))

            st.dataframe(
                df_ent,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "Entity Name": st.column_config.TextColumn("Entity Name", width="medium"),
                    "count": st.column_config.NumberColumn("Mentions", format="%d"),
                    "first_seen": st.column_config.DatetimeColumn("First Seen", format="YYYY-MM-DD HH:mm"),
                    "sources": st.column_config.ListColumn("Sources"),
                },
            )
        else:
            st.info("æœªæ‰¾åˆ°å®ä½“æ•°æ®ã€‚")

    with tab_events:
        col_evt_search, _ = st.columns([3, 1])
        with col_evt_search:
            event_search = st.text_input("ğŸ” Search Events", placeholder="e.g. ETF, Regulation...")
        events_data = utils.load_events()

        if events_data:
            df_evt = pd.DataFrame.from_dict(events_data, orient="index")
            df_evt["abstract"] = df_evt.index

            cols = ["abstract", "event_summary", "entities", "sources", "first_seen"]
            existing_cols = [c for c in cols if c in df_evt.columns]
            df_evt = df_evt[existing_cols]
            if "sources" in df_evt.columns:
                df_evt["sources"] = df_evt["sources"].apply(normalize_mixed)

            if event_search:
                mask = df_evt["abstract"].str.contains(event_search, case=False, na=False) | df_evt[
                    "event_summary"
                ].str.contains(event_search, case=False, na=False)
                df_evt = df_evt[mask]

            st.dataframe(
                df_evt,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "abstract": st.column_config.TextColumn("Event Abstract", width="medium"),
                    "event_summary": st.column_config.TextColumn("Summary", width="large"),
                    "entities": st.column_config.ListColumn("Involved Entities"),
                    "first_seen": st.column_config.DatetimeColumn("Detected At", format="YYYY-MM-DD"),
                },
            )
        else:
            st.info("æœªæ‰¾åˆ°äº‹ä»¶æ•°æ®ã€‚")

    with tab_news:
        c_file, c_view = st.columns([1, 3])

        with c_file:
            st.subheader("ğŸ“ Select File")
            files = utils.get_raw_news_files()
            if files:
                files = sorted(files, key=lambda x: x.stat().st_mtime, reverse=True)
                selected_file = st.radio(
                    "Available Files", files, format_func=lambda x: x.name, label_visibility="collapsed"
                )
            else:
                st.warning("æœªæ‰¾åˆ°æ–‡ä»¶ã€‚")
                selected_file = None

        with c_view:
            if selected_file:
                st.subheader(f"ğŸ“„ Content: {selected_file.name}")
                news_items = utils.load_raw_news_file(selected_file)

                if news_items:
                    items_per_page = 10
                    total_pages = max(1, (len(news_items) + items_per_page - 1) // items_per_page)

                    if "news_page" not in st.session_state:
                        st.session_state.news_page = 1

                    col_pg1, col_pg2, col_pg3 = st.columns([1, 2, 1])
                    with col_pg1:
                        if st.button("Previous", disabled=st.session_state.news_page <= 1):
                            st.session_state.news_page -= 1
                            st.rerun()
                    with col_pg2:
                        st.write(
                            f"Page {st.session_state.news_page} of {total_pages} (Total: {len(news_items)})"
                        )
                    with col_pg3:
                        if st.button("Next", disabled=st.session_state.news_page >= total_pages):
                            st.session_state.news_page += 1
                            st.rerun()

                    start_idx = (st.session_state.news_page - 1) * items_per_page
                    end_idx = start_idx + items_per_page
                    page_items = news_items[start_idx:end_idx]

                    for item in page_items:
                        title = item.get("title", "No Title")
                        date = item.get("datetime") or item.get("formatted_time") or "Unknown Date"
                        source = item.get("source", "Unknown Source")
                        content = item.get("content", "")

                        with st.expander(f"**{title}** | {source} | {date}"):
                            st.markdown(f"**Content:**\n{content}")
                            st.json(item, expanded=False)
                else:
                    st.info("æ–‡ä»¶ä¸ºç©ºã€‚")

    with tab_tmp:
        st.subheader("ğŸ—ƒï¸ Extracted Events Snapshots (tmp)")
        tmp_dir = PROJECT_ROOT / "data" / "tmp"
        files = sorted(tmp_dir.glob("extracted_events_*.jsonl"), key=lambda x: x.stat().st_mtime, reverse=True)

        if not files:
            st.info("æœªæ‰¾åˆ°æå–çš„å¿«ç…§æ–‡ä»¶ã€‚")
        else:
            data = []
            for f in files:
                try:
                    count = sum(1 for _ in f.open("r", encoding="utf-8"))
                except Exception:
                    count = 0
                data.append({"file": f.name, "rows": count, "path": str(f)})
            df_snap = pd.DataFrame(data)
            st.dataframe(df_snap, hide_index=True, use_container_width=True)

            selected = st.selectbox("é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶ï¼ˆä»…åˆ é™¤ tmp å¿«ç…§ï¼‰", [""] + [f.name for f in files])
            if selected and st.button("ğŸ—‘ï¸ åˆ é™¤æ‰€é€‰å¿«ç…§", type="primary"):
                try:
                    target = tmp_dir / selected
                    if target.exists():
                        target.unlink()
                        st.success(f"å·²åˆ é™¤ {selected}")
                        st.rerun()
                except Exception as e:
                    st.error(f"åˆ é™¤å¤±è´¥: {e}")

            st.divider()
            preview_file = st.selectbox("é€‰æ‹©è¦é¢„è§ˆçš„å¿«ç…§æ–‡ä»¶", [""] + [f.name for f in files], index=0)
            if preview_file:
                target = tmp_dir / preview_file
                try:
                    rows = []
                    with open(target, "r", encoding="utf-8") as f:
                        for idx, line in enumerate(f):
                            if idx >= 50:
                                break
                            try:
                                obj = json.loads(line)
                                rows.append(
                                    {
                                        "abstract": obj.get("abstract") or obj.get("event_summary") or "",
                                        "event_summary": obj.get("event_summary", ""),
                                        "entities": normalize_mixed(obj.get("entities")),
                                        "source": obj.get("source", ""),
                                        "published_at": obj.get("published_at", ""),
                                        "news_id": obj.get("news_id", ""),
                                    }
                                )
                            except Exception:
                                continue
                    if rows:
                        df_preview = pd.DataFrame(rows)
                        st.write(f"é¢„è§ˆ {preview_file} ï¼ˆæœ€å¤š 50 è¡Œï¼‰")
                        st.dataframe(df_preview, hide_index=True, use_container_width=True)
                    else:
                        st.info("æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è§£æå¯å±•ç¤ºå­—æ®µã€‚")
                except Exception as e:
                    st.error(f"é¢„è§ˆå¤±è´¥: {e}")


